{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Conductor is a Workflow Orchestration engine that runs in the cloud. Motivation We built Conductor to help us orchestrate microservices based process flows at Netflix with the following features: A distributed server ecosystem, which stores workflow state information efficiently. Allow creation of process / business flows in which each individual task can be implemented by the same / different microservices. A JSON DSL based blueprint defines the execution flow. Provide visibility and traceability into these process flows. Simple interface to connect workers, which execute the tasks in workflows. Full operational control over workflows with the ability to pause, resume, restart, retry and terminate. Allow greater reuse of existing microservices providing an easier path for onboarding. User interface to visualize, replay and search the process flows. Ability to scale to millions of concurrently running process flows. Backed by a queuing service abstracted from the clients. Be able to operate on HTTP or other transports e.g. gRPC. Event handlers to control workflows via external actions. Client implementations in Java, Python and other languages. Various configurable properties with sensible defaults to fine tune workflow and task executions like rate limiting, concurrent execution limits etc. Why not peer to peer choreography? With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach: Process flows are \u201cembedded\u201d within the code of multiple application. Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs. Almost no way to systematically answer \u201cHow much are we done with process X\u201d?","title":"Introduction"},{"location":"#motivation","text":"We built Conductor to help us orchestrate microservices based process flows at Netflix with the following features: A distributed server ecosystem, which stores workflow state information efficiently. Allow creation of process / business flows in which each individual task can be implemented by the same / different microservices. A JSON DSL based blueprint defines the execution flow. Provide visibility and traceability into these process flows. Simple interface to connect workers, which execute the tasks in workflows. Full operational control over workflows with the ability to pause, resume, restart, retry and terminate. Allow greater reuse of existing microservices providing an easier path for onboarding. User interface to visualize, replay and search the process flows. Ability to scale to millions of concurrently running process flows. Backed by a queuing service abstracted from the clients. Be able to operate on HTTP or other transports e.g. gRPC. Event handlers to control workflows via external actions. Client implementations in Java, Python and other languages. Various configurable properties with sensible defaults to fine tune workflow and task executions like rate limiting, concurrent execution limits etc. Why not peer to peer choreography? With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach: Process flows are \u201cembedded\u201d within the code of multiple application. Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs. Almost no way to systematically answer \u201cHow much are we done with process X\u201d?","title":"Motivation"},{"location":"apispec/","text":"Task Workflow Metadata Endpoint Description Input GET /metadata/taskdefs Get all the task definitions n/a GET /metadata/taskdefs/{taskType} Retrieve task definition Task Name POST /metadata/taskdefs Register new task definitions List of Task Definitions PUT /metadata/taskdefs Update a task definition A Task Definition DELETE /metadata/taskdefs/{taskType} Delete a task definition Task Name GET /metadata/workflow Get all the workflow definitions n/a POST /metadata/workflow Register new workflow Workflow Definition PUT /metadata/workflow Register/Update new workflows List of Workflow Definition GET /metadata/workflow/{name}?version= Get the workflow definitions workflow name, version (optional) Start A Workflow With Input only See Start Workflow Request . Output Id of the workflow (GUID) With Input and Task Domains POST /workflow { //JSON payload for Start workflow request } Start workflow request JSON for start workflow request { name : myWorkflow , // Name of the workflow version : 1, // Version \u201ccorrelationId\u201d: \u201ccorr1\u201d, // correlation Id priority : 1, // Priority input : { // Input map. }, taskToDomain : { // Task to domain map } } Output Id of the workflow (GUID) Retrieve Workflows Endpoint Description GET /workflow/{workflowId}?includeTasks=true|false Get Workflow State by workflow Id. If includeTasks is set, then also includes all the tasks executed and scheduled. GET /workflow/running/{name} Get all the running workflows of a given type GET /workflow/running/{name}/correlated/{correlationId}?includeClosed=true|false includeTasks=true|false Get all the running workflows filtered by correlation Id. If includeClosed is set, also includes workflows that have completed running. GET /workflow/search Search for workflows. See Below. Search for Workflows Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs. GET /workflow/search?start= size= sort= freeText= query= Parameter Description start Page number. Defaults to 0 size Number of results to return sort Sorting. Format is: ASC: fieldname or DESC: fieldname to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause. e.g. workflowType = 'name_of_workflow'. Optional if freeText is provided. Output Search result as described below: { totalHits : 0, results : [ { workflowType : string , version : 0, workflowId : string , correlationId : string , startTime : string , updateTime : string , endTime : string , status : RUNNING , input : string , output : string , reasonForIncompletion : string , executionTime : 0, event : string } ] } Manage Workflows Endpoint Description PUT /workflow/{workflowId}/pause Pause. No further tasks will be scheduled until resumed. Currently running tasks are not paused. PUT /workflow/{workflowId}/resume Resume normal operations after a pause. POST /workflow/{workflowId}/rerun See Below. POST /workflow/{workflowId}/restart Restart workflow execution from the start. Current execution history is wiped out. POST /workflow/{workflowId}/retry Retry the last failed task. PUT /workflow/{workflowId}/skiptask/{taskReferenceName} See below. DELETE /workflow/{workflowId} Terminates the running workflow. DELETE /workflow/{workflowId}/remove Deletes the workflow from system. Use with caution. Rerun Re-runs a completed workflow from a specific task. POST /workflow/{workflowId}/rerun { reRunFromWorkflowId : string , workflowInput : {}, reRunFromTaskId : string , taskInput : {} } Skip Task Skips a task execution (specified as taskReferenceName parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. PUT /workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId= taskReferenceName= { taskInput : {}, taskOutput : {} } Manage Tasks Endpoint Description GET /tasks/{taskId} Get task details. GET /tasks/queue/all List the pending task sizes. GET /tasks/queue/all/verbose Same as above, includes the size per shard GET /tasks/queue/sizes?taskType= taskType= taskType Return the size of pending tasks for given task types Polling, Ack and Update Task These are critical endpoints used to poll for task, send ack (after polling) and finally updating the task result by worker. Endpoint Description GET /tasks/poll/{taskType}?workerid= domain= Poll for a task. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain GET /tasks/poll/batch/{taskType}?count= timeout= workerid= domain Poll for a task in a batch specified by count . This is a long poll and the connection will wait until timeout or if there is at-least 1 item available, whichever comes first. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain POST /tasks Update the result of task execution. See the schema below. POST /tasks/{taskId}/ack Acknowledges the task received AFTER poll by worker. Schema for updating Task Result { workflowInstanceId : Workflow Instance Id , taskId : ID of the task to be updated , reasonForIncompletion : If failed, reason for failure , callbackAfterSeconds : 0, status : IN_PROGRESS|FAILED|COMPLETED , outputData : { //JSON document representing Task execution output } } Acknowledging tasks after poll If the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.","title":"API Specification"},{"location":"apispec/#task-workflow-metadata","text":"Endpoint Description Input GET /metadata/taskdefs Get all the task definitions n/a GET /metadata/taskdefs/{taskType} Retrieve task definition Task Name POST /metadata/taskdefs Register new task definitions List of Task Definitions PUT /metadata/taskdefs Update a task definition A Task Definition DELETE /metadata/taskdefs/{taskType} Delete a task definition Task Name GET /metadata/workflow Get all the workflow definitions n/a POST /metadata/workflow Register new workflow Workflow Definition PUT /metadata/workflow Register/Update new workflows List of Workflow Definition GET /metadata/workflow/{name}?version= Get the workflow definitions workflow name, version (optional)","title":"Task &amp; Workflow Metadata"},{"location":"apispec/#start-a-workflow","text":"","title":"Start A Workflow"},{"location":"apispec/#with-input-only","text":"See Start Workflow Request .","title":"With Input only"},{"location":"apispec/#output","text":"Id of the workflow (GUID)","title":"Output"},{"location":"apispec/#with-input-and-task-domains","text":"POST /workflow { //JSON payload for Start workflow request }","title":"With Input and Task Domains"},{"location":"apispec/#start-workflow-request","text":"JSON for start workflow request { name : myWorkflow , // Name of the workflow version : 1, // Version \u201ccorrelationId\u201d: \u201ccorr1\u201d, // correlation Id priority : 1, // Priority input : { // Input map. }, taskToDomain : { // Task to domain map } }","title":"Start workflow request"},{"location":"apispec/#output_1","text":"Id of the workflow (GUID)","title":"Output"},{"location":"apispec/#retrieve-workflows","text":"Endpoint Description GET /workflow/{workflowId}?includeTasks=true|false Get Workflow State by workflow Id. If includeTasks is set, then also includes all the tasks executed and scheduled. GET /workflow/running/{name} Get all the running workflows of a given type GET /workflow/running/{name}/correlated/{correlationId}?includeClosed=true|false includeTasks=true|false Get all the running workflows filtered by correlation Id. If includeClosed is set, also includes workflows that have completed running. GET /workflow/search Search for workflows. See Below.","title":"Retrieve Workflows"},{"location":"apispec/#search-for-workflows","text":"Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs. GET /workflow/search?start= size= sort= freeText= query= Parameter Description start Page number. Defaults to 0 size Number of results to return sort Sorting. Format is: ASC: fieldname or DESC: fieldname to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause. e.g. workflowType = 'name_of_workflow'. Optional if freeText is provided.","title":"Search for Workflows"},{"location":"apispec/#output_2","text":"Search result as described below: { totalHits : 0, results : [ { workflowType : string , version : 0, workflowId : string , correlationId : string , startTime : string , updateTime : string , endTime : string , status : RUNNING , input : string , output : string , reasonForIncompletion : string , executionTime : 0, event : string } ] }","title":"Output"},{"location":"apispec/#manage-workflows","text":"Endpoint Description PUT /workflow/{workflowId}/pause Pause. No further tasks will be scheduled until resumed. Currently running tasks are not paused. PUT /workflow/{workflowId}/resume Resume normal operations after a pause. POST /workflow/{workflowId}/rerun See Below. POST /workflow/{workflowId}/restart Restart workflow execution from the start. Current execution history is wiped out. POST /workflow/{workflowId}/retry Retry the last failed task. PUT /workflow/{workflowId}/skiptask/{taskReferenceName} See below. DELETE /workflow/{workflowId} Terminates the running workflow. DELETE /workflow/{workflowId}/remove Deletes the workflow from system. Use with caution.","title":"Manage Workflows"},{"location":"apispec/#rerun","text":"Re-runs a completed workflow from a specific task. POST /workflow/{workflowId}/rerun { reRunFromWorkflowId : string , workflowInput : {}, reRunFromTaskId : string , taskInput : {} }","title":"Rerun"},{"location":"apispec/#skip-task","text":"Skips a task execution (specified as taskReferenceName parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. PUT /workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId= taskReferenceName= { taskInput : {}, taskOutput : {} }","title":"Skip Task"},{"location":"apispec/#manage-tasks","text":"Endpoint Description GET /tasks/{taskId} Get task details. GET /tasks/queue/all List the pending task sizes. GET /tasks/queue/all/verbose Same as above, includes the size per shard GET /tasks/queue/sizes?taskType= taskType= taskType Return the size of pending tasks for given task types","title":"Manage Tasks"},{"location":"apispec/#polling-ack-and-update-task","text":"These are critical endpoints used to poll for task, send ack (after polling) and finally updating the task result by worker. Endpoint Description GET /tasks/poll/{taskType}?workerid= domain= Poll for a task. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain GET /tasks/poll/batch/{taskType}?count= timeout= workerid= domain Poll for a task in a batch specified by count . This is a long poll and the connection will wait until timeout or if there is at-least 1 item available, whichever comes first. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain POST /tasks Update the result of task execution. See the schema below. POST /tasks/{taskId}/ack Acknowledges the task received AFTER poll by worker.","title":"Polling, Ack and Update Task"},{"location":"apispec/#schema-for-updating-task-result","text":"{ workflowInstanceId : Workflow Instance Id , taskId : ID of the task to be updated , reasonForIncompletion : If failed, reason for failure , callbackAfterSeconds : 0, status : IN_PROGRESS|FAILED|COMPLETED , outputData : { //JSON document representing Task execution output } } Acknowledging tasks after poll If the worker fails to ack the task after polling, the task is re-queued and put back in queue and is made available during subsequent poll.","title":"Schema for updating Task Result"},{"location":"architecture/","text":"High Level Architecture The API and storage layers are pluggable and provide ability to work with different backends and queue service providers. Installing and Running Running in production For a detailed configuration guide on installing and running Conductor server in production visit Conductor Server documentation. Running In-Memory Server Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor. !!!warning: In-Memory server is meant for a quick demonstration purpose and does not store the data on disk. All the data is lost once the server dies. Checkout the source from github git clone git@github.com:Netflix/conductor.git Start Local Server The server is in the directory conductor/server . cd server ../gradlew server # wait for the server to come online Swagger APIs can be accessed at http://localhost:8080/ Start UI Server The UI Server is in the directory conductor/ui . To run it, you need Node.js installed and gulp installed with npm i -g gulp . In a terminal other than the one running the Conductor server: cd ui npm i gulp watch If you get an error message ReferenceError: primordials is not defined , you need to use an earlier version of Node (pre-12). See this issue for more details . Or Start all the services using docker-compose cd docker docker-compose up If you ran it locally, launch UI at http://localhost:3000/ OR if you ran it using docker-compose launch the UI at http://localhost:5000/ Note The server will load a sample kitchensink workflow definition by default. See here for details. Runtime Model Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues. Notes Workers are remote systems and communicates over HTTP with the conductor servers. Task Queues are used to schedule tasks for workers. We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism. conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend. See section under extending backend for implementing support for different databases for storage and indexing. High Level Steps Steps required for a new workflow to be registered and get executed: Define task definitions used by the workflow. Create the workflow definition Create task worker(s) that polls for scheduled tasks at regular interval Trigger Workflow Execution POST /workflow/{name} { ... //json payload as workflow input } Polling for a task GET /tasks/poll/batch/{taskType} Update task status POST /tasks { outputData : { encodeResult : success , location : http://cdn.example.com/file/location.png //any task specific output }, status : COMPLETED }","title":"Architecture"},{"location":"architecture/#high-level-architecture","text":"The API and storage layers are pluggable and provide ability to work with different backends and queue service providers.","title":"High Level Architecture"},{"location":"architecture/#installing-and-running","text":"Running in production For a detailed configuration guide on installing and running Conductor server in production visit Conductor Server documentation.","title":"Installing and Running"},{"location":"architecture/#running-in-memory-server","text":"Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor. !!!warning: In-Memory server is meant for a quick demonstration purpose and does not store the data on disk. All the data is lost once the server dies.","title":"Running In-Memory Server"},{"location":"architecture/#checkout-the-source-from-github","text":"git clone git@github.com:Netflix/conductor.git","title":"Checkout the source from github"},{"location":"architecture/#start-local-server","text":"The server is in the directory conductor/server . cd server ../gradlew server # wait for the server to come online Swagger APIs can be accessed at http://localhost:8080/","title":"Start Local Server"},{"location":"architecture/#start-ui-server","text":"The UI Server is in the directory conductor/ui . To run it, you need Node.js installed and gulp installed with npm i -g gulp . In a terminal other than the one running the Conductor server: cd ui npm i gulp watch If you get an error message ReferenceError: primordials is not defined , you need to use an earlier version of Node (pre-12). See this issue for more details .","title":"Start UI Server"},{"location":"architecture/#or-start-all-the-services-using-docker-compose","text":"cd docker docker-compose up If you ran it locally, launch UI at http://localhost:3000/ OR if you ran it using docker-compose launch the UI at http://localhost:5000/ Note The server will load a sample kitchensink workflow definition by default. See here for details.","title":"Or Start all the services using docker-compose"},{"location":"architecture/#runtime-model","text":"Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues. Notes Workers are remote systems and communicates over HTTP with the conductor servers. Task Queues are used to schedule tasks for workers. We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism. conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend. See section under extending backend for implementing support for different databases for storage and indexing.","title":"Runtime Model"},{"location":"architecture/#high-level-steps","text":"Steps required for a new workflow to be registered and get executed: Define task definitions used by the workflow. Create the workflow definition Create task worker(s) that polls for scheduled tasks at regular interval Trigger Workflow Execution POST /workflow/{name} { ... //json payload as workflow input } Polling for a task GET /tasks/poll/batch/{taskType} Update task status POST /tasks { outputData : { encodeResult : success , location : http://cdn.example.com/file/location.png //any task specific output }, status : COMPLETED }","title":"High Level Steps"},{"location":"bestpractices/","text":"Response Timeout Configure the responseTimeoutSeconds of each task to be 0. Should be less than or equal to timeoutSeconds. Payload sizes Configure your workflows such that conductor is not used as a persistence store. Ensure that the output data in the task result set in your worker is used by your workflow for execution. If the values in the output payloads are not used by subsequent tasks in your workflow, this data should not be sent back to conductor in the task result. In cases where the output data of your task is used within subsequent tasks in your workflow but is substantially large ( 100KB), consider uploading this data to an object store (S3 or similar) and set the location to the object in your task output. The subsequent tasks can then download this data from the given location and use it during execution.","title":"Best Practices"},{"location":"bestpractices/#response-timeout","text":"Configure the responseTimeoutSeconds of each task to be 0. Should be less than or equal to timeoutSeconds.","title":"Response Timeout"},{"location":"bestpractices/#payload-sizes","text":"Configure your workflows such that conductor is not used as a persistence store. Ensure that the output data in the task result set in your worker is used by your workflow for execution. If the values in the output payloads are not used by subsequent tasks in your workflow, this data should not be sent back to conductor in the task result. In cases where the output data of your task is used within subsequent tasks in your workflow but is substantially large ( 100KB), consider uploading this data to an object store (S3 or similar) and set the location to the object in your task output. The subsequent tasks can then download this data from the given location and use it during execution.","title":"Payload sizes"},{"location":"extend/","text":"Backend Conductor provides a pluggable backend. The current implementation uses Dynomite. There are 4 interfaces that needs to be implemented for each backend: //Store for workflow and task definitions com.netflix.conductor.dao.MetadataDAO //Store for workflow executions com.netflix.conductor.dao.ExecutionDAO //Index for workflow executions com.netflix.conductor.dao.IndexDAO //Queue provider for tasks com.netflix.conductor.dao.QueueDAO It is possible to mix and match different implementations for each of these. For example, SQS for queueing and a relational store for others. System Tasks To create system tasks follow the steps below: Extend com.netflix.conductor.core.execution.tasks.WorkflowSystemTask Instantiate the new class as part of the startup (eager singleton) Implement the TaskMapper interface Add this implementation to the map identified by TaskMappers External Payload Storage To configure conductor to externalize the storage of large payloads: Implement the ExternalPayloadStorage interface . Add the storage option to the enum here . Set this JVM system property workflow.external.payload.storage to the value of the enum element added above. Add a binding similar to this . Workflow Status Listener To provide a notification mechanism upon completion/termination of workflows: Implement the WorkflowStatusListener interface This can be configured to plugin custom notification/eventing upon workflows reaching a terminal state. Locking Service By default, Conductor Server module loads Zookeeper lock module. If you'd like to provide your own locking implementation module, for eg., with Dynomite and Redlock: Implement Lock interface. Add a binding similar to this Enable locking service: decider.locking.enabled: true","title":"Extending Conductor"},{"location":"extend/#backend","text":"Conductor provides a pluggable backend. The current implementation uses Dynomite. There are 4 interfaces that needs to be implemented for each backend: //Store for workflow and task definitions com.netflix.conductor.dao.MetadataDAO //Store for workflow executions com.netflix.conductor.dao.ExecutionDAO //Index for workflow executions com.netflix.conductor.dao.IndexDAO //Queue provider for tasks com.netflix.conductor.dao.QueueDAO It is possible to mix and match different implementations for each of these. For example, SQS for queueing and a relational store for others.","title":"Backend"},{"location":"extend/#system-tasks","text":"To create system tasks follow the steps below: Extend com.netflix.conductor.core.execution.tasks.WorkflowSystemTask Instantiate the new class as part of the startup (eager singleton) Implement the TaskMapper interface Add this implementation to the map identified by TaskMappers","title":"System Tasks"},{"location":"extend/#external-payload-storage","text":"To configure conductor to externalize the storage of large payloads: Implement the ExternalPayloadStorage interface . Add the storage option to the enum here . Set this JVM system property workflow.external.payload.storage to the value of the enum element added above. Add a binding similar to this .","title":"External Payload Storage"},{"location":"extend/#workflow-status-listener","text":"To provide a notification mechanism upon completion/termination of workflows: Implement the WorkflowStatusListener interface This can be configured to plugin custom notification/eventing upon workflows reaching a terminal state.","title":"Workflow Status Listener"},{"location":"extend/#locking-service","text":"By default, Conductor Server module loads Zookeeper lock module. If you'd like to provide your own locking implementation module, for eg., with Dynomite and Redlock: Implement Lock interface. Add a binding similar to this Enable locking service: decider.locking.enabled: true","title":"Locking Service"},{"location":"externalpayloadstorage/","text":"Warning The external payload storage is currently only implemented to be used to by the Java client. Client libraries in other languages need to be modified to enable this. Contributions are welcomed. Context Conductor can be configured to enforce barriers on the size of workflow and task payloads for both input and output. These barriers can be used as safeguards to prevent the usage of conductor as a data persistence system and to reduce the pressure on its datastore. Barriers Conductor typically applies two kinds of barriers: Soft Barrier Hard Barrier Soft Barrier The soft barrier is used to alleviate pressure on the conductor datastore. In some special workflow use-cases, the size of the payload is warranted enough to be stored as part of the workflow execution. In such cases, conductor externalizes the storage of such payloads to S3 and uploads/downloads to/from S3 as needed during the execution. This process is completely transparent to the user/worker process. Hard Barrier The hard barriers are enforced to safeguard the conductor backend from the pressure of having to persist and deal with voluminous data which is not essential for workflow execution. In such cases, conductor will reject such payloads and will terminate/fail the workflow execution with the reasonForIncompletion set to an appropriate error message detailing the payload size. Usage Barriers setup Set the following properties to the desired values in the JVM system properties: Property Description default value conductor.workflow.input.payload.threshold.kb Soft barrier for workflow input payload in KB 5120 conductor.max.workflow.input.payload.threshold.kb Hard barrier for workflow input payload in KB 10240 conductor.workflow.output.payload.threshold.kb Soft barrier for workflow output payload in KB 5120 conductor.max.workflow.output.payload.threshold.kb Hard barrier for workflow output payload in KB 10240 conductor.task.input.payload.threshold.kb Soft barrier for task input payload in KB 3072 conductor.max.task.input.payload.threshold.kb Hard barrier for task input payload in KB 10240 conductor.task.output.payload.threshold.kb Soft barrier for task output payload in KB 3072 conductor.max.task.output.payload.threshold.kb Hard barrier for task output payload in KB 10240 Amazon S3 Conductor provides an implementation of Amazon S3 used to externalize large payload storage. Set the following property in the JVM system properties: workflow.external.payload.storage=S3 Note This implementation assumes that S3 access is configured on the instance. Set the following properties to the desired values in the JVM system properties: Property Description default value workflow.external.payload.storage.s3.bucket S3 bucket where the payloads will be stored workflow.external.payload.storage.s3.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 The payloads will be stored in the bucket configured above in a UUID.json file at locations determined by the type of the payload. See here for information about how the object key is determined. Azure Blob Storage ProductLive provides an implementation of Azure Blob Storage used to externalize large payload storage. To build conductor with azure blob feature read the README.md in azureblob-storage module Note This implementation assumes that you have an Azure Blob Storage account's connection string or SAS Token . If you want signed url to expired you must specify a Connection String. Set the following properties to the desired values in the JVM system properties: Property Description default value workflow.external.payload.storage.azure_blob.connection_string Azure Blob Storage connection string. Required to sign Url. workflow.external.payload.storage.azure_blob.endpoint Azure Blob Storage endpoint. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.sas_token Azure Blob Storage SAS Token. Must have permissions Read and Write on Resource Object on Service Blob . Optional if connection_string is set. workflow.external.payload.storage.azure_blob.container_name Azure Blob Storage container where the payloads will be stored conductor-payloads workflow.external.payload.storage.azure_blob.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 workflow.external.payload.storage.azure_blob.workflow_input_path Path prefix where workflows input will be stored with an random UUID filename workflow/input/ workflow.external.payload.storage.azure_blob.workflow_output_path Path prefix where workflows output will be stored with an random UUID filename workflow/output/ workflow.external.payload.storage.azure_blob.task_input_path Path prefix where tasks input will be stored with an random UUID filename task/input/ workflow.external.payload.storage.azure_blob.task_output_path Path prefix where tasks output will be stored with an random UUID filename task/output/ The payloads will be stored as done in Amazon S3 .","title":"External Payload Storage"},{"location":"externalpayloadstorage/#context","text":"Conductor can be configured to enforce barriers on the size of workflow and task payloads for both input and output. These barriers can be used as safeguards to prevent the usage of conductor as a data persistence system and to reduce the pressure on its datastore.","title":"Context"},{"location":"externalpayloadstorage/#barriers","text":"Conductor typically applies two kinds of barriers: Soft Barrier Hard Barrier","title":"Barriers"},{"location":"externalpayloadstorage/#soft-barrier","text":"The soft barrier is used to alleviate pressure on the conductor datastore. In some special workflow use-cases, the size of the payload is warranted enough to be stored as part of the workflow execution. In such cases, conductor externalizes the storage of such payloads to S3 and uploads/downloads to/from S3 as needed during the execution. This process is completely transparent to the user/worker process.","title":"Soft Barrier"},{"location":"externalpayloadstorage/#hard-barrier","text":"The hard barriers are enforced to safeguard the conductor backend from the pressure of having to persist and deal with voluminous data which is not essential for workflow execution. In such cases, conductor will reject such payloads and will terminate/fail the workflow execution with the reasonForIncompletion set to an appropriate error message detailing the payload size.","title":"Hard Barrier"},{"location":"externalpayloadstorage/#usage","text":"","title":"Usage"},{"location":"externalpayloadstorage/#barriers-setup","text":"Set the following properties to the desired values in the JVM system properties: Property Description default value conductor.workflow.input.payload.threshold.kb Soft barrier for workflow input payload in KB 5120 conductor.max.workflow.input.payload.threshold.kb Hard barrier for workflow input payload in KB 10240 conductor.workflow.output.payload.threshold.kb Soft barrier for workflow output payload in KB 5120 conductor.max.workflow.output.payload.threshold.kb Hard barrier for workflow output payload in KB 10240 conductor.task.input.payload.threshold.kb Soft barrier for task input payload in KB 3072 conductor.max.task.input.payload.threshold.kb Hard barrier for task input payload in KB 10240 conductor.task.output.payload.threshold.kb Soft barrier for task output payload in KB 3072 conductor.max.task.output.payload.threshold.kb Hard barrier for task output payload in KB 10240","title":"Barriers setup"},{"location":"externalpayloadstorage/#amazon-s3","text":"Conductor provides an implementation of Amazon S3 used to externalize large payload storage. Set the following property in the JVM system properties: workflow.external.payload.storage=S3 Note This implementation assumes that S3 access is configured on the instance. Set the following properties to the desired values in the JVM system properties: Property Description default value workflow.external.payload.storage.s3.bucket S3 bucket where the payloads will be stored workflow.external.payload.storage.s3.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 The payloads will be stored in the bucket configured above in a UUID.json file at locations determined by the type of the payload. See here for information about how the object key is determined.","title":"Amazon S3"},{"location":"externalpayloadstorage/#azure-blob-storage","text":"ProductLive provides an implementation of Azure Blob Storage used to externalize large payload storage. To build conductor with azure blob feature read the README.md in azureblob-storage module Note This implementation assumes that you have an Azure Blob Storage account's connection string or SAS Token . If you want signed url to expired you must specify a Connection String. Set the following properties to the desired values in the JVM system properties: Property Description default value workflow.external.payload.storage.azure_blob.connection_string Azure Blob Storage connection string. Required to sign Url. workflow.external.payload.storage.azure_blob.endpoint Azure Blob Storage endpoint. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.sas_token Azure Blob Storage SAS Token. Must have permissions Read and Write on Resource Object on Service Blob . Optional if connection_string is set. workflow.external.payload.storage.azure_blob.container_name Azure Blob Storage container where the payloads will be stored conductor-payloads workflow.external.payload.storage.azure_blob.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 workflow.external.payload.storage.azure_blob.workflow_input_path Path prefix where workflows input will be stored with an random UUID filename workflow/input/ workflow.external.payload.storage.azure_blob.workflow_output_path Path prefix where workflows output will be stored with an random UUID filename workflow/output/ workflow.external.payload.storage.azure_blob.task_input_path Path prefix where tasks input will be stored with an random UUID filename task/input/ workflow.external.payload.storage.azure_blob.task_output_path Path prefix where tasks output will be stored with an random UUID filename task/output/ The payloads will be stored as done in Amazon S3 .","title":"Azure Blob Storage"},{"location":"faq/","text":"How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.) After polling for the task update the status of the task to IN_PROGRESS and set the callbackAfterSeconds value to the desired time. The task will remain in the queue until the specified second before worker polling for it will receive it again. If there is a timeout set for the task, and the callbackAfterSeconds exceeds the timeout value, it will result in task being TIMED_OUT. How long can a workflow be in running state? Can I have a workflow that keeps running for days or months? Yes. As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state. My workflow fails to start with missing task error Ensure all the tasks are registered via /metadata/taskdefs APIs. Add any missing task definition (as reported in the error) and try again. Where does my worker run? How does conductor run my tasks? Conductor does not run the workers. When a task is scheduled, it is put into the queue maintained by Conductor. Workers are required to poll for tasks using /tasks/poll API at periodic interval, execute the business logic for the task and report back the results using POST /tasks API call. Conductor, however will run system tasks on the Conductor server. How can I schedule workflows to run at a specific time? Conductor does not provide any scheduling mechanism. But you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow. Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing . How do I setup Dynomite cluster? Visit Dynomite's Github page to find details on setup and support mechanism. Can I use conductor with Ruby / Go / Python? Yes. Workers can be written any language as long as they can poll and update the task results via HTTP endpoints. Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server. Note: Python and Go clients have been contributed by the community. How can I get help with Dynomite? Visit Dynomite's Github page to find details on setup and support mechanism. My workflow is running and the task is SCHEDULED but it is not being processed. Make sure that the worker is actively polling for this task. Navigate to the Poll Data tab on the Condictor UI and search for your task name in the search box on the top right corner. Ensure that Last Poll Time for this task is current and the Last Polled By is an active instance. The Size column shows the number of scheduled tasks for this task name. How do I configure a notification when my workflow completes or fails? Refer this documentation to extend conductor to send out events/notifications upon workflow completion/failure. I want my worker to stop polling and executing tasks when the process is being terminated. (Java client) In a PreDestroy block within your application, call the shutdown() method on the TaskRunnerConfigurer instance that you have created to facilitate a graceful shutdown of your worker in case the process is being terminated. Can I exit early from a task without executing the configured automatic retries in the task definition? Set the status to FAILED_WITH_TERMINAL_ERROR in the TaskResult object within your worker. This would mark the task as FAILED and fail the workflow without retrying the task as a fail-fast mechanism.","title":"FAQ"},{"location":"faq/#how-do-you-schedule-a-task-to-be-put-in-the-queue-after-some-time-eg-1-hour-1-day-etc","text":"After polling for the task update the status of the task to IN_PROGRESS and set the callbackAfterSeconds value to the desired time. The task will remain in the queue until the specified second before worker polling for it will receive it again. If there is a timeout set for the task, and the callbackAfterSeconds exceeds the timeout value, it will result in task being TIMED_OUT.","title":"How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.)"},{"location":"faq/#how-long-can-a-workflow-be-in-running-state-can-i-have-a-workflow-that-keeps-running-for-days-or-months","text":"Yes. As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state.","title":"How long can a workflow be in running state?  Can I have a workflow that keeps running for days or months?"},{"location":"faq/#my-workflow-fails-to-start-with-missing-task-error","text":"Ensure all the tasks are registered via /metadata/taskdefs APIs. Add any missing task definition (as reported in the error) and try again.","title":"My workflow fails to start with missing task error"},{"location":"faq/#where-does-my-worker-run-how-does-conductor-run-my-tasks","text":"Conductor does not run the workers. When a task is scheduled, it is put into the queue maintained by Conductor. Workers are required to poll for tasks using /tasks/poll API at periodic interval, execute the business logic for the task and report back the results using POST /tasks API call. Conductor, however will run system tasks on the Conductor server.","title":"Where does my worker run?  How does conductor run my tasks?"},{"location":"faq/#how-can-i-schedule-workflows-to-run-at-a-specific-time","text":"Conductor does not provide any scheduling mechanism. But you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow. Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing .","title":"How can I schedule workflows to run at a specific time?"},{"location":"faq/#how-do-i-setup-dynomite-cluster","text":"Visit Dynomite's Github page to find details on setup and support mechanism.","title":"How do I setup Dynomite cluster?"},{"location":"faq/#can-i-use-conductor-with-ruby-go-python","text":"Yes. Workers can be written any language as long as they can poll and update the task results via HTTP endpoints. Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server. Note: Python and Go clients have been contributed by the community.","title":"Can I use conductor with Ruby / Go / Python?"},{"location":"faq/#how-can-i-get-help-with-dynomite","text":"Visit Dynomite's Github page to find details on setup and support mechanism.","title":"How can I get help with Dynomite?"},{"location":"faq/#my-workflow-is-running-and-the-task-is-scheduled-but-it-is-not-being-processed","text":"Make sure that the worker is actively polling for this task. Navigate to the Poll Data tab on the Condictor UI and search for your task name in the search box on the top right corner. Ensure that Last Poll Time for this task is current and the Last Polled By is an active instance. The Size column shows the number of scheduled tasks for this task name.","title":"My workflow is running and the task is SCHEDULED but it is not being processed."},{"location":"faq/#how-do-i-configure-a-notification-when-my-workflow-completes-or-fails","text":"Refer this documentation to extend conductor to send out events/notifications upon workflow completion/failure.","title":"How do I configure a notification when my workflow completes or fails?"},{"location":"faq/#i-want-my-worker-to-stop-polling-and-executing-tasks-when-the-process-is-being-terminated-java-client","text":"In a PreDestroy block within your application, call the shutdown() method on the TaskRunnerConfigurer instance that you have created to facilitate a graceful shutdown of your worker in case the process is being terminated.","title":"I want my worker to stop polling and executing tasks when the process is being terminated. (Java client)"},{"location":"faq/#can-i-exit-early-from-a-task-without-executing-the-configured-automatic-retries-in-the-task-definition","text":"Set the status to FAILED_WITH_TERMINAL_ERROR in the TaskResult object within your worker. This would mark the task as FAILED and fail the workflow without retrying the task as a fail-fast mechanism.","title":"Can I exit early from a task without executing the configured automatic retries in the task definition?"},{"location":"license/","text":"Copyright 2018 Netflix, Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"server/","text":"Installing Requirements Database : Dynomite Indexing Backend : Elasticsearch 5.x Servlet Container : Tomcat, Jetty, or similar running JDK 1.8 or higher There are 3 ways in which you can install Conductor: 1. Build from source To build from source, checkout the code from github and build server module using gradle build command. If you do not have gradle installed, you can run the command ./gradlew build from the project root. This produces conductor-server-all-VERSION.jar in the folder ./server/build/libs/ The jar can be executed using: java -jar conductor-server-VERSION-all.jar 2. Download pre-built binaries from jcenter or maven central Use the following coordinates: group artifact version com.netflix.conductor conductor-server-all 2.7.+ 3. Use the pre-configured Docker image To build the docker images for the conductor server and ui run the commands: cd docker docker-compose build After the docker images are built, run the following command to start the containers: docker-compose up This will create a docker container network that consists of the following images: conductor:server, conductor:ui, elasticsearch:5.6.8 , and dynomite. To view the UI, navigate to localhost:5000 , to view the Swagger docs, navigate to localhost:8080 . Configuration Conductor server uses a property file based configuration. The property file is passed to the Main class as a command line argument. java -jar conductor-server-all-VERSION.jar [PATH TO PROPERTY FILE] [log4j.properties file path] log4j.properties file path is optional and allows finer control over the logging (defaults to INFO level logging in the console). Configuration Parameters # Database persistence model. Possible values are memory, redis, redis_cluster, redis_sentinel and dynomite. # If omitted, the persistence used is memory # # memory : The data is stored in memory and lost when the server dies. Useful for testing or demo # redis : non-Dynomite based redis instance # redis_cluster: AWS Elasticache Redis (cluster mode enabled).See [http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Clusters.Create.CON.RedisCluster.html] # redis_sentinel: Redis HA with Redis Sentinel. See [https://redis.io/topics/sentinel] # dynomite : Dynomite cluster. Use this for HA configuration. db=dynomite # Dynomite Cluster details. # format is host:port:rack separated by semicolon # for AWS Elasticache Redis (cluster mode enabled) the format is configuration_endpoint:port:us-east-1e. The region in this case does not matter workflow.dynomite.cluster.hosts=host1:8102:us-east-1c;host2:8102:us-east-1d;host3:8102:us-east-1e # If you are running using dynomite, also add the following line to the property # to set the rack/availability zone of the conductor server to be same as dynomite cluster config EC2_AVAILABILTY_ZONE=us-east-1c # Dynomite cluster name workflow.dynomite.cluster.name=dyno_cluster_name # Maximum connections to redis/dynomite workflow.dynomite.connection.maxConnsPerHost=31 # Namespace for the keys stored in Dynomite/Redis workflow.namespace.prefix=conductor # Namespace prefix for the dyno queues workflow.namespace.queue.prefix=conductor_queues # No. of threads allocated to dyno-queues (optional) queues.dynomite.threads=10 # Non-quorum port used to connect to local redis. Used by dyno-queues. # When using redis directly, set this to the same port as redis server. # For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite. queues.dynomite.nonQuorum.port=22122 # Transport address to elasticsearch # Specifying multiple node urls is not supported. specify one of the nodes' url, or a load balancer. workflow.elasticsearch.url=localhost:9300 # Name of the elasticsearch cluster workflow.elasticsearch.index.name=conductor # Additional modules (optional) conductor.additional.modules=class_extending_com.google.inject.AbstractModule High Availability Configuration Conductor servers are stateless and can be deployed on multiple servers to handle scale and availability needs. The scalability of the server is achieved by scaling the Dynomite cluster along with dyno-queues which is used for queues. Clients connects to the server via HTTP load balancer or using Discovery (on NetflixOSS stack). Using Standalone Redis / ElastiCache Conductor server can be used with a standlone Redis or ElastiCache server. To configure the server, change the config to use the following: db=redis # For AWS Elasticache Redis (cluster mode enabled) the format is configuration_endpoint:port:us-east-1e. # The region in this case does not matter workflow.dynomite.cluster.hosts=server_address:server_port:us-east-1e workflow.dynomite.connection.maxConnsPerHost=31 queues.dynomite.nonQuorum.port=server_port Setting up Zookeeper to enable Distributed Locking Service. See Technical Details for more details about this. Locking Service is disabled by default. Enable this by setting: decider.locking.enabled: true Setup Zookeeper cluster connection string: zk.connection=1.2.3.4:2181,5.6.7.8:2181 Optionally, configure the default timeouts: zk.sessionTimeoutMs zk.connectionTimeoutMs Default Workflow Archiving Module Configuration Conductor server does not perform automated workflow execution data cleaning by default. Archiving module (if enabled) removes all execution data from conductor persistence storage immediately upon workflow completion or termination, but keeps archived index data in elastic search. To benefit form archiving module you have to do the following: 1. Enable Archiving Module Set property in server configuration. # Comma-separated additional conductor modules conductor.additional.modules=com.netflix.conductor.contribs.ArchivingWorkflowModule 2. Enable Workflow Status Listener Archiving module is triggered only if workflow status listener is enabled on workflow definition level. To enable it you have to set workflowStatusListenerEnabled property to true . See sample workflow definition below: { name : e2e_approval_v4 , description : Approval Process , workflowStatusListenerEnabled : true, tasks : [] }","title":"Conductor Server"},{"location":"server/#installing","text":"","title":"Installing"},{"location":"server/#requirements","text":"Database : Dynomite Indexing Backend : Elasticsearch 5.x Servlet Container : Tomcat, Jetty, or similar running JDK 1.8 or higher There are 3 ways in which you can install Conductor:","title":"Requirements"},{"location":"server/#1-build-from-source","text":"To build from source, checkout the code from github and build server module using gradle build command. If you do not have gradle installed, you can run the command ./gradlew build from the project root. This produces conductor-server-all-VERSION.jar in the folder ./server/build/libs/ The jar can be executed using: java -jar conductor-server-VERSION-all.jar","title":"1. Build from source"},{"location":"server/#2-download-pre-built-binaries-from-jcenter-or-maven-central","text":"Use the following coordinates: group artifact version com.netflix.conductor conductor-server-all 2.7.+","title":"2. Download pre-built binaries from jcenter or maven central"},{"location":"server/#3-use-the-pre-configured-docker-image","text":"To build the docker images for the conductor server and ui run the commands: cd docker docker-compose build After the docker images are built, run the following command to start the containers: docker-compose up This will create a docker container network that consists of the following images: conductor:server, conductor:ui, elasticsearch:5.6.8 , and dynomite. To view the UI, navigate to localhost:5000 , to view the Swagger docs, navigate to localhost:8080 .","title":"3. Use the pre-configured Docker image"},{"location":"server/#configuration","text":"Conductor server uses a property file based configuration. The property file is passed to the Main class as a command line argument. java -jar conductor-server-all-VERSION.jar [PATH TO PROPERTY FILE] [log4j.properties file path] log4j.properties file path is optional and allows finer control over the logging (defaults to INFO level logging in the console).","title":"Configuration"},{"location":"server/#configuration-parameters","text":"# Database persistence model. Possible values are memory, redis, redis_cluster, redis_sentinel and dynomite. # If omitted, the persistence used is memory # # memory : The data is stored in memory and lost when the server dies. Useful for testing or demo # redis : non-Dynomite based redis instance # redis_cluster: AWS Elasticache Redis (cluster mode enabled).See [http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Clusters.Create.CON.RedisCluster.html] # redis_sentinel: Redis HA with Redis Sentinel. See [https://redis.io/topics/sentinel] # dynomite : Dynomite cluster. Use this for HA configuration. db=dynomite # Dynomite Cluster details. # format is host:port:rack separated by semicolon # for AWS Elasticache Redis (cluster mode enabled) the format is configuration_endpoint:port:us-east-1e. The region in this case does not matter workflow.dynomite.cluster.hosts=host1:8102:us-east-1c;host2:8102:us-east-1d;host3:8102:us-east-1e # If you are running using dynomite, also add the following line to the property # to set the rack/availability zone of the conductor server to be same as dynomite cluster config EC2_AVAILABILTY_ZONE=us-east-1c # Dynomite cluster name workflow.dynomite.cluster.name=dyno_cluster_name # Maximum connections to redis/dynomite workflow.dynomite.connection.maxConnsPerHost=31 # Namespace for the keys stored in Dynomite/Redis workflow.namespace.prefix=conductor # Namespace prefix for the dyno queues workflow.namespace.queue.prefix=conductor_queues # No. of threads allocated to dyno-queues (optional) queues.dynomite.threads=10 # Non-quorum port used to connect to local redis. Used by dyno-queues. # When using redis directly, set this to the same port as redis server. # For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite. queues.dynomite.nonQuorum.port=22122 # Transport address to elasticsearch # Specifying multiple node urls is not supported. specify one of the nodes' url, or a load balancer. workflow.elasticsearch.url=localhost:9300 # Name of the elasticsearch cluster workflow.elasticsearch.index.name=conductor # Additional modules (optional) conductor.additional.modules=class_extending_com.google.inject.AbstractModule","title":"Configuration Parameters"},{"location":"server/#high-availability-configuration","text":"Conductor servers are stateless and can be deployed on multiple servers to handle scale and availability needs. The scalability of the server is achieved by scaling the Dynomite cluster along with dyno-queues which is used for queues. Clients connects to the server via HTTP load balancer or using Discovery (on NetflixOSS stack).","title":"High Availability Configuration"},{"location":"server/#using-standalone-redis-elasticache","text":"Conductor server can be used with a standlone Redis or ElastiCache server. To configure the server, change the config to use the following: db=redis # For AWS Elasticache Redis (cluster mode enabled) the format is configuration_endpoint:port:us-east-1e. # The region in this case does not matter workflow.dynomite.cluster.hosts=server_address:server_port:us-east-1e workflow.dynomite.connection.maxConnsPerHost=31 queues.dynomite.nonQuorum.port=server_port","title":"Using Standalone Redis / ElastiCache"},{"location":"server/#setting-up-zookeeper-to-enable-distributed-locking-service","text":"See Technical Details for more details about this. Locking Service is disabled by default. Enable this by setting: decider.locking.enabled: true Setup Zookeeper cluster connection string: zk.connection=1.2.3.4:2181,5.6.7.8:2181 Optionally, configure the default timeouts: zk.sessionTimeoutMs zk.connectionTimeoutMs","title":"Setting up Zookeeper to enable Distributed Locking Service."},{"location":"server/#default-workflow-archiving-module-configuration","text":"Conductor server does not perform automated workflow execution data cleaning by default. Archiving module (if enabled) removes all execution data from conductor persistence storage immediately upon workflow completion or termination, but keeps archived index data in elastic search. To benefit form archiving module you have to do the following:","title":"Default Workflow Archiving Module Configuration"},{"location":"server/#1-enable-archiving-module","text":"Set property in server configuration. # Comma-separated additional conductor modules conductor.additional.modules=com.netflix.conductor.contribs.ArchivingWorkflowModule","title":"1. Enable Archiving Module"},{"location":"server/#2-enable-workflow-status-listener","text":"Archiving module is triggered only if workflow status listener is enabled on workflow definition level. To enable it you have to set workflowStatusListenerEnabled property to true . See sample workflow definition below: { name : e2e_approval_v4 , description : Approval Process , workflowStatusListenerEnabled : true, tasks : [] }","title":"2. Enable Workflow Status Listener"},{"location":"tasklifecycle/","text":"Task state transitions The figure below depicts the state transitions that a task can go through within a workflow execution. Retries and Failure Scenarios Task failure and retries Retries for failed task executions of each task can be configured independently. retryCount, retryDelaySeconds and retryLogic can be used to configure the retry mechanism. Worker (W1) polls for task T1 from the Conductor server and receives the task. Upon processing this task, the worker determines that the task execution is a failure and reports this to the server with FAILED status after 10 seconds. The server will persist this FAILED execution of T1. A new execution of task T1 will be created and scheduled to be polled. This task will be available to be polled after 5 (retryDelaySeconds) seconds. Timeout seconds Timeout is the maximum amount of time that the task must reach a terminal state in, else the task will be marked as TIMED_OUT. 0 seconds - Worker polls for task T1 fom the Conductor server and receives the task. T1 is put into IN_PROGRESS status by the server. Worker starts processing the task but is unable to process the task at this time. Worker updates the server with T1 set to IN_PROGRESS status and a callback of 9 seconds. Server puts T1 back in the queue but makes it invisible and the worker continues to poll for the task but does not receive T1 for 9 seconds. 9,18 seconds - Worker receives T1 from the server and is still unable to process the task and updates the server with a callback of 9 seconds. 27 seconds - Worker polls and receives task T1 from the server and is now able to process this task. 30 seconds (T1 timeout) - Server marks T1 as TIMED_OUT because it is not in a terminal state after first being moved to IN_PROGRESS status. Server schedules a new task based on the retry count. 32 seconds - Worker completes processing of T1 and updates the server with COMPLETED status. Server will ignore this update since T1 has already been moved to a terminal status (TIMED_OUT). Response timeout seconds Response timeout is the time within which the worker must respond to the server with an update for the task, else the task will be marked as TIMED_OUT. 0 seconds - Worker polls for the task T1 from the Conductor server and receives the task. T1 is put into IN_PROGRESS status by the server. Worker starts processing the task but the worker instance dies during this execution. 20 seconds (T1 responseTimeout) - Server marks T1 as TIMED_OUT since the task has not been updated by the worker within the configured responseTimeoutSeconds (20). A new instance of task T1 is scheduled as per the retry configuration. 25 seconds - The retried instance of T1 is available to be polled by the worker, after the retryDelaySeconds (5) has elapsed.","title":"Task Lifecycle"},{"location":"tasklifecycle/#task-state-transitions","text":"The figure below depicts the state transitions that a task can go through within a workflow execution.","title":"Task state transitions"},{"location":"tasklifecycle/#retries-and-failure-scenarios","text":"","title":"Retries and Failure Scenarios"},{"location":"tasklifecycle/#task-failure-and-retries","text":"Retries for failed task executions of each task can be configured independently. retryCount, retryDelaySeconds and retryLogic can be used to configure the retry mechanism. Worker (W1) polls for task T1 from the Conductor server and receives the task. Upon processing this task, the worker determines that the task execution is a failure and reports this to the server with FAILED status after 10 seconds. The server will persist this FAILED execution of T1. A new execution of task T1 will be created and scheduled to be polled. This task will be available to be polled after 5 (retryDelaySeconds) seconds.","title":"Task failure and retries"},{"location":"tasklifecycle/#timeout-seconds","text":"Timeout is the maximum amount of time that the task must reach a terminal state in, else the task will be marked as TIMED_OUT. 0 seconds - Worker polls for task T1 fom the Conductor server and receives the task. T1 is put into IN_PROGRESS status by the server. Worker starts processing the task but is unable to process the task at this time. Worker updates the server with T1 set to IN_PROGRESS status and a callback of 9 seconds. Server puts T1 back in the queue but makes it invisible and the worker continues to poll for the task but does not receive T1 for 9 seconds. 9,18 seconds - Worker receives T1 from the server and is still unable to process the task and updates the server with a callback of 9 seconds. 27 seconds - Worker polls and receives task T1 from the server and is now able to process this task. 30 seconds (T1 timeout) - Server marks T1 as TIMED_OUT because it is not in a terminal state after first being moved to IN_PROGRESS status. Server schedules a new task based on the retry count. 32 seconds - Worker completes processing of T1 and updates the server with COMPLETED status. Server will ignore this update since T1 has already been moved to a terminal status (TIMED_OUT).","title":"Timeout seconds"},{"location":"tasklifecycle/#response-timeout-seconds","text":"Response timeout is the time within which the worker must respond to the server with an update for the task, else the task will be marked as TIMED_OUT. 0 seconds - Worker polls for the task T1 from the Conductor server and receives the task. T1 is put into IN_PROGRESS status by the server. Worker starts processing the task but the worker instance dies during this execution. 20 seconds (T1 responseTimeout) - Server marks T1 as TIMED_OUT since the task has not been updated by the worker within the configured responseTimeoutSeconds (20). A new instance of task T1 is scheduled as per the retry configuration. 25 seconds - The retried instance of T1 is available to be polled by the worker, after the retryDelaySeconds (5) has elapsed.","title":"Response timeout seconds"},{"location":"technicaldetails/","text":"gRPC Framework As part of this addition, all of the modules and bootstrap code within them were refactored to leverage providers, which facilitated moving the Jetty server into a separate module and the conformance to Guice guidelines and best practices. This feature constitutes a server-side gRPC implementation along with protobuf RPC schemas for the workflow, metadata and task APIs that can be run concurrently with the Jersey-based HTTP/REST server. The protobuf models for all the types are exposed through the API. gRPC java clients for the workflow, metadata and task APIs are also available for use. Another valuable addition is an idiomatic Go gRPC client implementation for the worker API. The proto models are auto-generated at compile time using this ProtoGen library. This custom library adds messageInput and messageOutput fields to all proto tasks and task definitions. The goal of these fields is providing a type-safe way to pass input and input metadata through tasks that use the gRPC API. These fields use the Any protobuf type which can store any arbitrary message type in a type-safe way, without the server needing to know the exact serialization format of the message. In order to expose these Any objects in the REST API, a custom encoding is used that contains the raw data of the serialized message by converting it into a dictionary with '@type' and '@value' keys, where '@type' is identical to the canonical representation and '@value' contains a base64 encoded string with the binary data of the serialized message. The JsonMapperProvider provides the object mapper initialized with this module to enable serialization/deserialization of these JSON objects. Cassandra Persistence The Cassandra persistence layer currently provides a partial implementation of the ExecutionDAO that supports all the CRUD operations for tasks and workflow execution. The data modelling is done in a denormalized manner and stored in two tables. The \u201cworkflows\u201d table houses all the information for a workflow execution including all its tasks and is the source of truth for all the information regarding a workflow and its tasks. The \u201ctask_lookup\u201d table, as the name suggests stores a lookup of taskIds to workflowId. This table facilitates the fast retrieval of task data given a taskId. All the datastore operations that are used during the critical execution path of a workflow have been implemented currently. Few of the operational abilities of the ExecutionDAO are yet to be implemented. This module also does not provide implementations for QueueDAO and MetadataDAO. We envision using the Cassandra DAO with an external queue implementation, since implementing a queuing recipe on top of Cassandra is an anti-pattern that we want to stay away from. External Payload Storage The implementation of this feature is such that the externalization of payloads is fully transparent and automated to the user. Conductor operators can configure the usage of this feature and is completely abstracted and hidden from the user, thereby allowing the operators full control over the barrier limits. Currently, only AWS S3 is supported as a storage system, however, as with all other Conductor components, this is pluggable and can be extended to enable any other object store to be used as an external payload storage system. The externalization of payloads is enforced using two kinds of barriers . Soft barriers are used when the payload size is warranted enough to be stored as part of workflow execution. These payloads will be stored in external storage and used during execution. Hard barriers are enforced to safeguard against voluminous data, and such payloads are rejected and the workflow execution is failed. The payload size is evaluated in the client before being sent over the wire to the server. If the payload size exceeds the configured soft limit, the client makes a request to the server for the location at which the payload is to be stored. In this case where S3 is being used, the server returns a signed url for the location and the client uploads the payload using this signed url. The relative path to the payload object is then stored in the workflow/task metadata. The server can then download this payload from this path and use as needed during execution. This allows the server to control access to the S3 bucket, thereby making the user applications where the worker processes are run completely agnostic of the permissions needed to access this location. Dynamic Workflow Executions In the earlier version (v1.x), Conductor allowed the execution of workflows referencing the workflow and task definitions stored as metadata in the system. This meant that a workflow execution with 10 custom tasks to run entailed: Registration of the 10 task definitions if they don't exist (assuming workflow task type SIMPLE for simplicity) Registration of the workflow definition Each time a definition needs to be retrieved, a call to the metadata store needed to be performed In addition to that, the system allowed current metadata that is in use to be altered, leading to possible inconsistencies/race conditions To eliminate these pain points, the execution was changed such that the workflow definition is embedded within the workflow execution and the task definitions are themselves embedded within this workflow definition. This enables the concept of ephemeral/dynamic workflows and tasks. Instead of fetching metadata definitions throughout the execution, the definitions are fetched and embedded into the execution at the start of the workflow execution. This also enabled the StartWorkflowRequest to be extended to provide the complete workflow definition that will be used during execution, thus removing the need for pre-registration. The MetadataMapperService prefetches the workflow and task definitions and embeds these within the workflow data, if not provided in the StartWorkflowRequest. Following benefits are seen as a result of these changes: Grants immutability of the definition stored within the execution data against modifications to the metadata store Better testability of workflows with faster experimental changes to definitions Reduced stress on the datastore due to prefetching the metadata only once at the start Decoupling Elasticsearch from Persistence In the earlier version (1.x), the indexing logic was imbibed within the persistence layer, thus creating a tight coupling between the primary datastore and the indexing engine. This meant that the primary datastore determines how we orchestrate between the storage (redis, mysql, etc) and the indexer(elastic search). The main disadvantage of this approach is the lack of flexibility, that is, we cannot run an in-memory database and external elastic search or vice-versa. We plan to improve this further by removing the indexing from the critical path of workflow execution, thus reducing possible points of failure during execution. Elasticsearch 5/6 Support Indexing workflow execution is one of the primary features of Conductor. This enables archival of terminal state workflows from the primary data store, along with providing a clean search capability from the UI. In Conductor 1.x, we supported both versions 2 and 5 of Elasticsearch by shadowing version 5 and all its dependencies. This proved to be rather tedious increasing build times by over 10 minutes. In Conductor 2.x, we have removed active support for ES 2.x, because of valuable community contributions for elasticsearch 5 and elasticsearch 6 modules. Unlike Conductor 1.x, Conductor 2.x supports elasticsearch 5 by default, which can easily be replaced with version 6 by following the simple instructions here . Maintaining workflow consistency with distributed locking and fencing tokens Problem Conductor\u2019s Workflow decide is the core logic which recursively evaluates the state of the workflow, schedules tasks, persists workflow and task(s) state at several checkpoints, and progresses the workflow. In a multi-node Conductor server deployment, the decide on a workflow can be triggered concurrently. For example, the worker can update Conductor server with latest task state, which calls decide, while the sweeper service (which periodically evaluates the workflow state to progress from task timeouts) would also call the decide on a different instance. The decide can be run concurrently in two different jvm nodes with two different workflow states, and based on the workflow configuration and current state, the result could be inconsistent. A two-part solution to maintain Workflow Consistency Preventing concurrent decides with distributed locking: The goal is to allow only one decide to run on a workflow at any given time across the whole Conductor Server cluster. This can be achieved by plugging in distributed locking implementations like Zookeeper, Redlock etc. A Zookeeper module implementing Conductor\u2019s Locking service is provided. Preventing stale data updates with fencing tokens: While the locking service helps to run one decide at a time, it might still be possible for nodes with timed out locks to reactivate and continue execution from where it left off (usually with stale data). This can be avoided with fencing tokens, which basically is an incrementing counter on workflow state with read-before-write support in a transaction or similar construct. At Netflix, we use Cassandra. Considering the tradeoffs of Cassandra\u2019s Lightweight Transactions (LWT) and the probability of this stale updates happening, and our testing results, we\u2019ve decided to first only rollout distributed locking with Zookeeper. We'll monitor our system and add C LWT if needed. Setting up desired level of consistency Based on your requirements, it is possible to use none, one or both of the distributed locking and fencing tokens implementations. Alternative solution to distributed \"decide\" evaluation As mentioned in the previous section, the \"decide\" logic is triggered from multiple places in a conductor instance. Either a direct trigger such as user starting a workflow or a timed trigger from the Sweeper service. Sweeper service is responsible for continually checking state of all workflows executions and trigger the \"decide\" logic which in turn can time the workflow out. In a single node deployment (single dynomite rack and single conductor server) this shouldn't be a problem. But when running multiple replicated dynomite racks and a conductor server on top of each rack, this might trigger the race condition described in previous section. Dynomite rack is a single or multiple instance dynomite setup that holds all the data. More on dynomite HA setup: (https://netflixtechblog.com/introducing-dynomite-making-non-distributed-databases-distributed-c7bce3d89404) In a cluster deployment, the default behavior for Dyno Queues is such, that it distributes the workload (round-robin style) to all the conductor servers. This can create a situation where the first task to be executed is queued for conductor server #1 but the sweeper service is queued for conductor server #2. More on dyno queues Dyno queues are the default queuing mechanism of conductor. Queues are allocated and used for: * Task execution - each task type gets a queue * Workflow execution - single queue with all currently executing workflows (deciderQueue) * This queue is used by SweeperService Each conductor server instance gets its own set of queues . Or more precisely a queue shard of its own. This means that if you have 2 task types, you end up with 6 queues altogether e.g. conductor_queues.test.QUEUE._deciderQueue.c conductor_queues.test.QUEUE._deciderQueue.d conductor_queues.test.QUEUE.HTTP.c conductor_queues.test.QUEUE.HTTP.d conductor_queues.test.QUEUE.LAMBDA.c conductor_queues.test.QUEUE.LAMBDA.d The \"c\" and \"d\" suffixes are the shards identifying conductor server instace #1 and instance #2 respectively. The shard names are extracted from dynomite rack name such as us-east-1c that is set in \"LOCAL_RACK\" or \"EC2_AVAILABILTY_ZONE\" Considering an execution of a simple workflow with just 2 tasks: [HTTP, LAMBDA], you should end up with queues being filled as follows: Workflow execution - conductor_queues.test.QUEUE._deciderQueue.c HTTP taks execution - conductor_queues.test.QUEUE.HTTP.d LAMBDA task execution - conductor_queues.test.QUEUE.LAMBDA.c Which means that SweeperService in conductor instance #1 is responsible for sweeping the workflow, conductor #2 is responsible for executing HTTP task and conductor #1 again responsible for executing LAMBDA task. This illustrates the race condition: If the HTTP task completion in instance #2 happens at the same time as sweep in instance #1 ... you can end up with 2 different updates to a workflow execution: one update timing workflow out while the other completing the task and scheduling next. The round-robin strategy responsible for work distribution is defined here Back to alternative solution The alternative solution here is Switching round-robin queue allocation for a local-only strategy . Meaning that a workflow and its task executions are queued only for the conductor instance which started the workflow. This completely avoids the race condition for the price of removing task execution distribution. Since all tasks and the sweeper service read/write only from/to \"local\" queues, it is impossible to run into a race condition between conductor instances. The downside here is that the workload is not distributed across all conductor servers. Which might be an advantage in active-standby deployments. Considering other downsides ... Considering a situation where a conductor instance goes down: * With local-only strategy, the workflow executions from failed conductor instance will not progress until: * The conductor instance is restarted or * The executions are manually terminated and restarted from a different node * With round-robin strategy, there is a chance the tasks will be rescheduled on a different conductor node * This is nondeterministic though Enabling local only queue allocation strategy for dyno queues: Just enable following setting the config.properties: workflow.dyno.queue.sharding.strategy=localOnly The default is roundRobin","title":"Technical Details"},{"location":"technicaldetails/#grpc-framework","text":"As part of this addition, all of the modules and bootstrap code within them were refactored to leverage providers, which facilitated moving the Jetty server into a separate module and the conformance to Guice guidelines and best practices. This feature constitutes a server-side gRPC implementation along with protobuf RPC schemas for the workflow, metadata and task APIs that can be run concurrently with the Jersey-based HTTP/REST server. The protobuf models for all the types are exposed through the API. gRPC java clients for the workflow, metadata and task APIs are also available for use. Another valuable addition is an idiomatic Go gRPC client implementation for the worker API. The proto models are auto-generated at compile time using this ProtoGen library. This custom library adds messageInput and messageOutput fields to all proto tasks and task definitions. The goal of these fields is providing a type-safe way to pass input and input metadata through tasks that use the gRPC API. These fields use the Any protobuf type which can store any arbitrary message type in a type-safe way, without the server needing to know the exact serialization format of the message. In order to expose these Any objects in the REST API, a custom encoding is used that contains the raw data of the serialized message by converting it into a dictionary with '@type' and '@value' keys, where '@type' is identical to the canonical representation and '@value' contains a base64 encoded string with the binary data of the serialized message. The JsonMapperProvider provides the object mapper initialized with this module to enable serialization/deserialization of these JSON objects.","title":"gRPC Framework"},{"location":"technicaldetails/#cassandra-persistence","text":"The Cassandra persistence layer currently provides a partial implementation of the ExecutionDAO that supports all the CRUD operations for tasks and workflow execution. The data modelling is done in a denormalized manner and stored in two tables. The \u201cworkflows\u201d table houses all the information for a workflow execution including all its tasks and is the source of truth for all the information regarding a workflow and its tasks. The \u201ctask_lookup\u201d table, as the name suggests stores a lookup of taskIds to workflowId. This table facilitates the fast retrieval of task data given a taskId. All the datastore operations that are used during the critical execution path of a workflow have been implemented currently. Few of the operational abilities of the ExecutionDAO are yet to be implemented. This module also does not provide implementations for QueueDAO and MetadataDAO. We envision using the Cassandra DAO with an external queue implementation, since implementing a queuing recipe on top of Cassandra is an anti-pattern that we want to stay away from.","title":"Cassandra Persistence"},{"location":"technicaldetails/#external-payload-storage","text":"The implementation of this feature is such that the externalization of payloads is fully transparent and automated to the user. Conductor operators can configure the usage of this feature and is completely abstracted and hidden from the user, thereby allowing the operators full control over the barrier limits. Currently, only AWS S3 is supported as a storage system, however, as with all other Conductor components, this is pluggable and can be extended to enable any other object store to be used as an external payload storage system. The externalization of payloads is enforced using two kinds of barriers . Soft barriers are used when the payload size is warranted enough to be stored as part of workflow execution. These payloads will be stored in external storage and used during execution. Hard barriers are enforced to safeguard against voluminous data, and such payloads are rejected and the workflow execution is failed. The payload size is evaluated in the client before being sent over the wire to the server. If the payload size exceeds the configured soft limit, the client makes a request to the server for the location at which the payload is to be stored. In this case where S3 is being used, the server returns a signed url for the location and the client uploads the payload using this signed url. The relative path to the payload object is then stored in the workflow/task metadata. The server can then download this payload from this path and use as needed during execution. This allows the server to control access to the S3 bucket, thereby making the user applications where the worker processes are run completely agnostic of the permissions needed to access this location.","title":"External Payload Storage"},{"location":"technicaldetails/#dynamic-workflow-executions","text":"In the earlier version (v1.x), Conductor allowed the execution of workflows referencing the workflow and task definitions stored as metadata in the system. This meant that a workflow execution with 10 custom tasks to run entailed: Registration of the 10 task definitions if they don't exist (assuming workflow task type SIMPLE for simplicity) Registration of the workflow definition Each time a definition needs to be retrieved, a call to the metadata store needed to be performed In addition to that, the system allowed current metadata that is in use to be altered, leading to possible inconsistencies/race conditions To eliminate these pain points, the execution was changed such that the workflow definition is embedded within the workflow execution and the task definitions are themselves embedded within this workflow definition. This enables the concept of ephemeral/dynamic workflows and tasks. Instead of fetching metadata definitions throughout the execution, the definitions are fetched and embedded into the execution at the start of the workflow execution. This also enabled the StartWorkflowRequest to be extended to provide the complete workflow definition that will be used during execution, thus removing the need for pre-registration. The MetadataMapperService prefetches the workflow and task definitions and embeds these within the workflow data, if not provided in the StartWorkflowRequest. Following benefits are seen as a result of these changes: Grants immutability of the definition stored within the execution data against modifications to the metadata store Better testability of workflows with faster experimental changes to definitions Reduced stress on the datastore due to prefetching the metadata only once at the start","title":"Dynamic Workflow Executions"},{"location":"technicaldetails/#decoupling-elasticsearch-from-persistence","text":"In the earlier version (1.x), the indexing logic was imbibed within the persistence layer, thus creating a tight coupling between the primary datastore and the indexing engine. This meant that the primary datastore determines how we orchestrate between the storage (redis, mysql, etc) and the indexer(elastic search). The main disadvantage of this approach is the lack of flexibility, that is, we cannot run an in-memory database and external elastic search or vice-versa. We plan to improve this further by removing the indexing from the critical path of workflow execution, thus reducing possible points of failure during execution.","title":"Decoupling Elasticsearch from Persistence"},{"location":"technicaldetails/#elasticsearch-56-support","text":"Indexing workflow execution is one of the primary features of Conductor. This enables archival of terminal state workflows from the primary data store, along with providing a clean search capability from the UI. In Conductor 1.x, we supported both versions 2 and 5 of Elasticsearch by shadowing version 5 and all its dependencies. This proved to be rather tedious increasing build times by over 10 minutes. In Conductor 2.x, we have removed active support for ES 2.x, because of valuable community contributions for elasticsearch 5 and elasticsearch 6 modules. Unlike Conductor 1.x, Conductor 2.x supports elasticsearch 5 by default, which can easily be replaced with version 6 by following the simple instructions here .","title":"Elasticsearch 5/6 Support"},{"location":"technicaldetails/#maintaining-workflow-consistency-with-distributed-locking-and-fencing-tokens","text":"","title":"Maintaining workflow consistency with distributed locking and fencing tokens"},{"location":"technicaldetails/#problem","text":"Conductor\u2019s Workflow decide is the core logic which recursively evaluates the state of the workflow, schedules tasks, persists workflow and task(s) state at several checkpoints, and progresses the workflow. In a multi-node Conductor server deployment, the decide on a workflow can be triggered concurrently. For example, the worker can update Conductor server with latest task state, which calls decide, while the sweeper service (which periodically evaluates the workflow state to progress from task timeouts) would also call the decide on a different instance. The decide can be run concurrently in two different jvm nodes with two different workflow states, and based on the workflow configuration and current state, the result could be inconsistent.","title":"Problem"},{"location":"technicaldetails/#a-two-part-solution-to-maintain-workflow-consistency","text":"Preventing concurrent decides with distributed locking: The goal is to allow only one decide to run on a workflow at any given time across the whole Conductor Server cluster. This can be achieved by plugging in distributed locking implementations like Zookeeper, Redlock etc. A Zookeeper module implementing Conductor\u2019s Locking service is provided. Preventing stale data updates with fencing tokens: While the locking service helps to run one decide at a time, it might still be possible for nodes with timed out locks to reactivate and continue execution from where it left off (usually with stale data). This can be avoided with fencing tokens, which basically is an incrementing counter on workflow state with read-before-write support in a transaction or similar construct. At Netflix, we use Cassandra. Considering the tradeoffs of Cassandra\u2019s Lightweight Transactions (LWT) and the probability of this stale updates happening, and our testing results, we\u2019ve decided to first only rollout distributed locking with Zookeeper. We'll monitor our system and add C LWT if needed.","title":"A two-part solution to maintain Workflow Consistency"},{"location":"technicaldetails/#setting-up-desired-level-of-consistency","text":"Based on your requirements, it is possible to use none, one or both of the distributed locking and fencing tokens implementations.","title":"Setting up desired level of consistency"},{"location":"technicaldetails/#alternative-solution-to-distributed-decide-evaluation","text":"As mentioned in the previous section, the \"decide\" logic is triggered from multiple places in a conductor instance. Either a direct trigger such as user starting a workflow or a timed trigger from the Sweeper service. Sweeper service is responsible for continually checking state of all workflows executions and trigger the \"decide\" logic which in turn can time the workflow out. In a single node deployment (single dynomite rack and single conductor server) this shouldn't be a problem. But when running multiple replicated dynomite racks and a conductor server on top of each rack, this might trigger the race condition described in previous section. Dynomite rack is a single or multiple instance dynomite setup that holds all the data. More on dynomite HA setup: (https://netflixtechblog.com/introducing-dynomite-making-non-distributed-databases-distributed-c7bce3d89404) In a cluster deployment, the default behavior for Dyno Queues is such, that it distributes the workload (round-robin style) to all the conductor servers. This can create a situation where the first task to be executed is queued for conductor server #1 but the sweeper service is queued for conductor server #2.","title":"Alternative solution to distributed \"decide\" evaluation"},{"location":"technicaldetails/#more-on-dyno-queues","text":"Dyno queues are the default queuing mechanism of conductor. Queues are allocated and used for: * Task execution - each task type gets a queue * Workflow execution - single queue with all currently executing workflows (deciderQueue) * This queue is used by SweeperService Each conductor server instance gets its own set of queues . Or more precisely a queue shard of its own. This means that if you have 2 task types, you end up with 6 queues altogether e.g. conductor_queues.test.QUEUE._deciderQueue.c conductor_queues.test.QUEUE._deciderQueue.d conductor_queues.test.QUEUE.HTTP.c conductor_queues.test.QUEUE.HTTP.d conductor_queues.test.QUEUE.LAMBDA.c conductor_queues.test.QUEUE.LAMBDA.d The \"c\" and \"d\" suffixes are the shards identifying conductor server instace #1 and instance #2 respectively. The shard names are extracted from dynomite rack name such as us-east-1c that is set in \"LOCAL_RACK\" or \"EC2_AVAILABILTY_ZONE\" Considering an execution of a simple workflow with just 2 tasks: [HTTP, LAMBDA], you should end up with queues being filled as follows: Workflow execution - conductor_queues.test.QUEUE._deciderQueue.c HTTP taks execution - conductor_queues.test.QUEUE.HTTP.d LAMBDA task execution - conductor_queues.test.QUEUE.LAMBDA.c Which means that SweeperService in conductor instance #1 is responsible for sweeping the workflow, conductor #2 is responsible for executing HTTP task and conductor #1 again responsible for executing LAMBDA task. This illustrates the race condition: If the HTTP task completion in instance #2 happens at the same time as sweep in instance #1 ... you can end up with 2 different updates to a workflow execution: one update timing workflow out while the other completing the task and scheduling next. The round-robin strategy responsible for work distribution is defined here","title":"More on dyno queues"},{"location":"technicaldetails/#back-to-alternative-solution","text":"The alternative solution here is Switching round-robin queue allocation for a local-only strategy . Meaning that a workflow and its task executions are queued only for the conductor instance which started the workflow. This completely avoids the race condition for the price of removing task execution distribution. Since all tasks and the sweeper service read/write only from/to \"local\" queues, it is impossible to run into a race condition between conductor instances. The downside here is that the workload is not distributed across all conductor servers. Which might be an advantage in active-standby deployments. Considering other downsides ... Considering a situation where a conductor instance goes down: * With local-only strategy, the workflow executions from failed conductor instance will not progress until: * The conductor instance is restarted or * The executions are manually terminated and restarted from a different node * With round-robin strategy, there is a chance the tasks will be rescheduled on a different conductor node * This is nondeterministic though Enabling local only queue allocation strategy for dyno queues: Just enable following setting the config.properties: workflow.dyno.queue.sharding.strategy=localOnly The default is roundRobin","title":"Back to alternative solution"},{"location":"configuration/eventhandlers/","text":"Introduction Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems. This includes: Being able to produce an event (message) in an external system like SQS or internal to Conductor. Start a workflow when a specific event occurs that matches the provided criteria. Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow. Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations. Event Task Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. See Event Task for documentation. Event Handler Event handlers are listeners registered that executes an action when a matching event occurs. The supported actions are: Start a Workflow Fail a Task Complete a Task Event Handlers can be configured to listen to Conductor Events or an external event like SQS. Configuration Event Handlers are configured via /event/ APIs. Structure: { name : descriptive unique name , event : event_type:event_location , condition : boolean condition , actions : [ see examples below ] } Condition Condition is an expression that MUST evaluate to a boolean value. A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to true . Examples Given the following payload in the message: { fileType : AUDIO , version : 3, metadata : { length : 300, codec : aac } } Expression Result $.version 1 true $.version 10 false $.metadata.length == 300 true Actions Start A Workflow { action : start_workflow , start_workflow : { name : WORKFLOW_NAME , version : optional_param , input : { param1 : ${param1} } } } Complete Task * { action : complete_task , complete_task : { workflowId : ${workflowId} , taskRefName : task_1 , output : { response : ${result} } }, expandInlineJSON : true } Fail Task * { action : fail_task , fail_task : { workflowId : ${workflowId} , taskRefName : task_1 , output : { response : ${result} } }, expandInlineJSON : true } Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring workflow inputs. Expanding stringified JSON elements in payload expandInlineJSON property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions. Extending Provide the implementation of EventQueueProvider . SQS Queue Provider: SQSEventQueueProvider.java","title":"Event Handlers"},{"location":"configuration/eventhandlers/#introduction","text":"Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems. This includes: Being able to produce an event (message) in an external system like SQS or internal to Conductor. Start a workflow when a specific event occurs that matches the provided criteria. Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow. Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations.","title":"Introduction"},{"location":"configuration/eventhandlers/#event-task","text":"Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. See Event Task for documentation.","title":"Event Task"},{"location":"configuration/eventhandlers/#event-handler","text":"Event handlers are listeners registered that executes an action when a matching event occurs. The supported actions are: Start a Workflow Fail a Task Complete a Task Event Handlers can be configured to listen to Conductor Events or an external event like SQS.","title":"Event Handler"},{"location":"configuration/eventhandlers/#configuration","text":"Event Handlers are configured via /event/ APIs.","title":"Configuration"},{"location":"configuration/eventhandlers/#structure","text":"{ name : descriptive unique name , event : event_type:event_location , condition : boolean condition , actions : [ see examples below ] }","title":"Structure:"},{"location":"configuration/eventhandlers/#condition","text":"Condition is an expression that MUST evaluate to a boolean value. A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to true . Examples Given the following payload in the message: { fileType : AUDIO , version : 3, metadata : { length : 300, codec : aac } } Expression Result $.version 1 true $.version 10 false $.metadata.length == 300 true","title":"Condition"},{"location":"configuration/eventhandlers/#actions","text":"Start A Workflow { action : start_workflow , start_workflow : { name : WORKFLOW_NAME , version : optional_param , input : { param1 : ${param1} } } } Complete Task * { action : complete_task , complete_task : { workflowId : ${workflowId} , taskRefName : task_1 , output : { response : ${result} } }, expandInlineJSON : true } Fail Task * { action : fail_task , fail_task : { workflowId : ${workflowId} , taskRefName : task_1 , output : { response : ${result} } }, expandInlineJSON : true } Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring workflow inputs. Expanding stringified JSON elements in payload expandInlineJSON property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions.","title":"Actions"},{"location":"configuration/eventhandlers/#extending","text":"Provide the implementation of EventQueueProvider . SQS Queue Provider: SQSEventQueueProvider.java","title":"Extending"},{"location":"configuration/isolationgroups/","text":"Isolation Group Id Consider an HTTP task where the latency of an API is high, task queue piles up effecting execution of other HTTP tasks which have low latency. We can isolate the execution of such tasks to have predictable performance using isolationgroupId , a property of task def. When we set isolationGroupId, the executor(SystemTaskWorkerCoordinator) will allocate an isolated queue and an isolated thread pool for execution of those tasks. If no isolationgroupId is specified in taskdef, then fallback is default behaviour where the executor executes the task in shared threadpool for all tasks. Example taskdef { name : encode_task , retryCount : 3, timeoutSeconds : 1200, inputKeys : [ sourceRequestId , qcElementType ], outputKeys : [ state , skipped , result ], timeoutPolicy : TIME_OUT_WF , retryLogic : FIXED , retryDelaySeconds : 600, responseTimeoutSeconds : 3600, concurrentExecLimit : 100, rateLimitFrequencyInSeconds : 60, rateLimitPerFrequency : 50, isolationgroupId : myIsolationGroupId } Example Workflow task { name : encode_and_deploy , description : Encodes a file and deploys to CDN , version : 1, tasks : [ { name : encode , taskReferenceName : encode , type : HTTP , inputParameters : { http_request : { uri : http://localhost:9200/conductor/_search?size=10 , method : GET } } } ], outputParameters : { cdn_url : ${d1.output.location} }, failureWorkflow : cleanup_encode_resources , restartable : true, workflowStatusListenerEnabled : true, schemaVersion : 2 } puts encode in HTTP-myIsolationGroupId queue, and allocates a new thread pool for this for execution. Note: To enable this feature, the workflow.isolated.system.task.enable property needs to be made true ,its default value is false The property workflow.isolated.system.task.worker.thread.count sets the thread pool size for isolated tasks; default is 1 . isolationGroupId is currently supported only in HTTP and kafka Task. Execution Name Space executionNameSpace A property of taskdef can be used to provide JVM isolation to task execution and scale executor deployments horizontally. Limitation of using isolationGroupId is that we need to scale executors vertically as the executor allocates a new thread pool per isolationGroupId . Also, since the executor runs the tasks in the same JVM, task execution is not isolated completely. To support JVM isolation, and also allow the executors to scale horizontally, we can use executionNameSpace property in taskdef. Executor consumes tasks whose executionNameSpace matches with the configuration property workflow.system.task.worker.executionNameSpace If the property is not set, the executor executes tasks without any executionNameSpace set. { name : encode_task , retryCount : 3, timeoutSeconds : 1200, inputKeys : [ sourceRequestId , qcElementType ], outputKeys : [ state , skipped , result ], timeoutPolicy : TIME_OUT_WF , retryLogic : FIXED , retryDelaySeconds : 600, responseTimeoutSeconds : 3600, concurrentExecLimit : 100, rateLimitFrequencyInSeconds : 60, rateLimitPerFrequency : 50, executionNameSpace : myExecutionNameSpace } Example Workflow task { name : encode_and_deploy , description : Encodes a file and deploys to CDN , version : 1, tasks : [ { name : encode , taskReferenceName : encode , type : HTTP , inputParameters : { http_request : { uri : http://localhost:9200/conductor/_search?size=10 , method : GET } } } ], outputParameters : { cdn_url : ${d1.output.location} }, failureWorkflow : cleanup_encode_resources , restartable : true, workflowStatusListenerEnabled : true, schemaVersion : 2 } encode task is executed by the executor deployment whose workflow.system.task.worker.executionNameSpace property is myExecutionNameSpace executionNameSpace can be used along with isolationGroupId If the above task contains a isolationGroupId myIsolationGroupId , the tasks will be scheduled in a queue HTTP@myExecutionNameSpace-myIsolationGroupId, and have a new threadpool for execution in the deployment group with myExecutionNameSpace","title":"Isolation Groups"},{"location":"configuration/isolationgroups/#isolation-group-id","text":"Consider an HTTP task where the latency of an API is high, task queue piles up effecting execution of other HTTP tasks which have low latency. We can isolate the execution of such tasks to have predictable performance using isolationgroupId , a property of task def. When we set isolationGroupId, the executor(SystemTaskWorkerCoordinator) will allocate an isolated queue and an isolated thread pool for execution of those tasks. If no isolationgroupId is specified in taskdef, then fallback is default behaviour where the executor executes the task in shared threadpool for all tasks. Example taskdef { name : encode_task , retryCount : 3, timeoutSeconds : 1200, inputKeys : [ sourceRequestId , qcElementType ], outputKeys : [ state , skipped , result ], timeoutPolicy : TIME_OUT_WF , retryLogic : FIXED , retryDelaySeconds : 600, responseTimeoutSeconds : 3600, concurrentExecLimit : 100, rateLimitFrequencyInSeconds : 60, rateLimitPerFrequency : 50, isolationgroupId : myIsolationGroupId } Example Workflow task { name : encode_and_deploy , description : Encodes a file and deploys to CDN , version : 1, tasks : [ { name : encode , taskReferenceName : encode , type : HTTP , inputParameters : { http_request : { uri : http://localhost:9200/conductor/_search?size=10 , method : GET } } } ], outputParameters : { cdn_url : ${d1.output.location} }, failureWorkflow : cleanup_encode_resources , restartable : true, workflowStatusListenerEnabled : true, schemaVersion : 2 } puts encode in HTTP-myIsolationGroupId queue, and allocates a new thread pool for this for execution. Note: To enable this feature, the workflow.isolated.system.task.enable property needs to be made true ,its default value is false The property workflow.isolated.system.task.worker.thread.count sets the thread pool size for isolated tasks; default is 1 . isolationGroupId is currently supported only in HTTP and kafka Task.","title":"Isolation Group Id"},{"location":"configuration/isolationgroups/#execution-name-space","text":"executionNameSpace A property of taskdef can be used to provide JVM isolation to task execution and scale executor deployments horizontally. Limitation of using isolationGroupId is that we need to scale executors vertically as the executor allocates a new thread pool per isolationGroupId . Also, since the executor runs the tasks in the same JVM, task execution is not isolated completely. To support JVM isolation, and also allow the executors to scale horizontally, we can use executionNameSpace property in taskdef. Executor consumes tasks whose executionNameSpace matches with the configuration property workflow.system.task.worker.executionNameSpace If the property is not set, the executor executes tasks without any executionNameSpace set. { name : encode_task , retryCount : 3, timeoutSeconds : 1200, inputKeys : [ sourceRequestId , qcElementType ], outputKeys : [ state , skipped , result ], timeoutPolicy : TIME_OUT_WF , retryLogic : FIXED , retryDelaySeconds : 600, responseTimeoutSeconds : 3600, concurrentExecLimit : 100, rateLimitFrequencyInSeconds : 60, rateLimitPerFrequency : 50, executionNameSpace : myExecutionNameSpace } Example Workflow task { name : encode_and_deploy , description : Encodes a file and deploys to CDN , version : 1, tasks : [ { name : encode , taskReferenceName : encode , type : HTTP , inputParameters : { http_request : { uri : http://localhost:9200/conductor/_search?size=10 , method : GET } } } ], outputParameters : { cdn_url : ${d1.output.location} }, failureWorkflow : cleanup_encode_resources , restartable : true, workflowStatusListenerEnabled : true, schemaVersion : 2 } encode task is executed by the executor deployment whose workflow.system.task.worker.executionNameSpace property is myExecutionNameSpace executionNameSpace can be used along with isolationGroupId If the above task contains a isolationGroupId myIsolationGroupId , the tasks will be scheduled in a queue HTTP@myExecutionNameSpace-myIsolationGroupId, and have a new threadpool for execution in the deployment group with myExecutionNameSpace","title":"Execution Name Space"},{"location":"configuration/systask/","text":"Decision A decision task is similar to case...switch statement in a programming language. The task takes 3 parameters: Parameters: name type description caseValueParam String Name of the parameter in task input whose value will be used as a switch. decisionCases Map[String, List[task]] Map where key is possible values of caseValueParam with value being list of tasks to be executed. defaultCase List[task] List of tasks to be executed when no matching value if found in decision case (default condition) caseExpression String Case expression to use instead of caseValueParam when the case should depend on complex values. This is a Javascript expression evaluated by the Nashorn Engine. Task names with arithmetic operators should not be used. Outputs: name type description caseOutput List[String] A List of string representing the list of cases that matched. Example { name : decide_task , taskReferenceName : decide1 , inputParameters : { case_value_param : ${workflow.input.movieType} }, type : DECISION , caseValueParam : case_value_param , decisionCases : { Show : [ { name : setup_episodes , taskReferenceName : se1 , inputParameters : { movieId : ${workflow.input.movieId} }, type : SIMPLE }, { name : generate_episode_artwork , taskReferenceName : ga , inputParameters : { movieId : ${workflow.input.movieId} }, type : SIMPLE } ], Movie : [ { name : setup_movie , taskReferenceName : sm , inputParameters : { movieId : ${workflow.input.movieId} }, type : SIMPLE }, { name : generate_movie_artwork , taskReferenceName : gma , inputParameters : { movieId : ${workflow.input.movieId} }, type : SIMPLE } ] } } Event Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. Parameters: name type description sink String Qualified name of the event that is produced. e.g. conductor or sqs:sqs_queue_name asyncComplete Boolean false to mark status COMPLETED upon execution ; true to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. Outputs: name type description workflowInstanceId String Workflow id workflowType String Workflow Name workflowVersion Integer Workflow Version correlationId String Workflow CorrelationId sink String Copy of the input data \"sink\" asyncComplete Boolean Copy of the input data \"asyncComplete event_produced String Name of the event produced The published event's payload is identical to the output of the task (except \"event_produced\"). Example { sink : sqs:example_sqs_queue_name , asyncComplete : false } When producing an event with Conductor as sink, the event name follows the structure: conductor: workflow_name : task_reference_name For SQS, use the name of the queue and NOT the URI. Conductor looks up the URI based on the name. Warning When using SQS add the ContribsModule to the deployment. The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs. Supported Sinks Conductor SQS HTTP An HTTP task is used to make calls to another microservice over HTTP. Parameters: name type description http_request HttpRequest JSON object (see below) HttpRequest JSON object: name type description uri String URI for the service. Can be a partial when using vipAddress or includes the server address. method String HTTP method. One of the GET, PUT, POST, DELETE, OPTIONS, HEAD accept String Accept header as required by server. Defaults to application/json contentType String Content Type - supported types are text/plain , text/html , and application/json (Default) headers Map[String, Any] A map of additional http headers to be sent along with the request. body Map[] Request body vipAddress String When using discovery based service URLs. asyncComplete Boolean false to mark status COMPLETED upon execution ; true to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. oauthConsumerKey String OAuth client consumer key oauthConsumerSecret String OAuth client consumer secret connectionTimeOut Integer Connection Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 100. readTimeOut Integer Read Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 150. Output: name type description response Map JSON body containing the response if one is present headers Map[String, Any] Response Headers statusCode Integer Http Status Code reasonPhrase String Http Status Code's reason phrase Example Task Input payload using vipAddress { http_request : { vipAddress : examplevip-prod , uri : / , method : GET , accept : text/plain } } Task Input using an absolute URL { http_request : { uri : http://example.com/ , method : GET , accept : text/plain } } The task is marked as FAILED if the request cannot be completed or the remote server returns non successful status code. Note HTTP task currently only supports Content-Type as application/json and is able to parse the text as well as JSON response. XML input/output is currently not supported. However, if the response cannot be parsed as JSON or Text, a string representation is stored as a text value. Sub Workflow Sub Workflow task allows for nesting a workflow within another workflow. Parameters: name type description subWorkflowParam Map[String, Any] See below subWorkflowParam name type description name String Name of the workflow to execute version Integer Version of the workflow to execute taskToDomain Map[String, String] Allows scheduling the sub workflow's tasks per given mappings. See Task Domains for instructions to configure taskDomains. workflowDefinition WorkflowDefinition Allows starting a subworkflow with a dynamic workflow definition. Outputs: name type description subWorkflowId String Subworkflow execution Id generated when running the subworkflow Example { name : sub_workflow_task , taskReferenceName : sub1 , type : SUB_WORKFLOW , inputParameters : { subWorkflowParam : { name : deployment_workflow , version : 1, taskToDomain : { * : mydomain }, workflowDefinition : { name : deployment_workflow , description : Deploys to CDN , version : 1, tasks : [{ name : deploy , taskReferenceName : d1 , type : SIMPLE , inputParameters : { fileLocation : ${workflow.input.encodeLocation} } }], outputParameters : { cdn_url : ${d1.output.location} }, failureWorkflow : cleanup_encode_resources , restartable : true, workflowStatusListenerEnabled : true, schemaVersion : 2 } }, anythingelse : value } } When executed, a deployment_workflow is executed with its inputs parameters set to the inputParameters of the sub_workflow_task and the workflow definition specified. The task is marked as completed upon the completion of the spawned workflow. If the sub-workflow is terminated or fails the task is marked as failure and retried if configured. Fork Fork is used to schedule parallel set of tasks, specified by \"type\":\"FORK_JOIN\" . Parameters: name description forkTasks A list of list of tasks. Each sublist is scheduled to be executed in parallel. However, tasks within the sublists are scheduled in a serial fashion. Example [ { name : fork_join , taskReferenceName : forkx , type : FORK_JOIN , forkTasks : [ [ { name : task_10 , taskReferenceName : task_A , type : SIMPLE }, { name : task_11 , taskReferenceName : task_B , type : SIMPLE } ], [ { name : task_21 , taskReferenceName : task_Y , type : SIMPLE }, { name : task_22 , taskReferenceName : task_Z , type : SIMPLE } ] ] }, { name : join , taskReferenceName : join2 , type : JOIN , joinOn : [ task_B , task_Z ] } ] When executed, task_A and task_Y are scheduled to be executed at the same time. Fork and Join A Join task MUST follow FORK_JOIN Workflow definition MUST include a Join task definition followed by FORK_JOIN task. Forked task can be a Sub Workflow, allowing for more complex execution flows. Dynamic Fork A dynamic fork is same as FORK_JOIN task. Except that the list of tasks to be forked is provided at runtime using task's input. Useful when number of tasks to be forked is not fixed and varies based on the input. name description dynamicForkTasksParam Name of the parameter that contains list of workflow task configuration to be executed in parallel dynamicForkTasksInputParamName Name of the parameter whose value should be a map with key as forked task's reference name and value as input the forked task Example { inputParameters : { dynamicTasks : ${taskA.output.dynamicTasksJSON} , dynamicTasksInput : ${taskA.output.dynamicTasksInputJSON} }, type : FORK_JOIN_DYNAMIC , dynamicForkTasksParam : dynamicTasks , dynamicForkTasksInputParamName : dynamicTasksInput } Consider taskA 's output as: { dynamicTasksInputJSON : { forkedTask1 : { width : 100, height : 100, params : { recipe : jpg } }, forkedTask2 : { width : 200, height : 200, params : { recipe : jpg } } }, dynamicTasksJSON : [ { name : encode_task , taskReferenceName : forkedTask1 , type : SIMPLE }, { name : encode_task , taskReferenceName : forkedTask2 , type : SIMPLE } ] } When executed, the dynamic fork task will schedule two parallel task of type \"encode_task\" with reference names \"forkedTask1\" and \"forkedTask2\" and inputs as specified by _ dynamicTasksInputJSON_ Dynamic Fork and Join A Join task MUST follow FORK_JOIN_DYNAMIC Workflow definition MUST include a Join task definition followed by FORK_JOIN_DYNAMIC task. However, given the dynamic nature of the task, no joinOn parameters are required for this Join. The join will wait for ALL the forked branches to complete before completing. Unlike FORK, which can execute parallel flows with each fork executing a series of tasks in sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork. However, forked task can be a Sub Workflow, allowing for more complex execution flows. Join Join task is used to wait for completion of one or more tasks spawned by fork tasks. Parameters: name description joinOn List of task reference name, for which the JOIN will wait for completion. Example { joinOn : [ taskRef1 , taskRef3 ] } Join Task Output Fork task's output will be a JSON object with key being the task reference name and value as the output of the fork task. Exclusive Join Exclusive Join task helps capture Task output from Decision Task's flow. For example, If we have a Workflow with T1 - [Decision: T2/T3] - EJ, then based on the decision, Exclusive Join (EJ) will produce the output from T2 or T3. I.e What ever is the output of one of T2/T3 will be available to downstream tasks through Exclusive Join task. If Decision Task takes True/False as cases, then: True: T1 - T2 - EJ; EJ will have T2's output. False: T1 - T3 - EJ; EJ will have T3's output. Undefined: T1 - EJ; EJ will have T1's output. Parameters: name description joinOn List of task reference names, which the EXCLUSIVE_JOIN will lookout for to capture output. From above example, this could be [\"T2\", \"T3\"] defaultExclusiveJoinTask Task reference name, whose output should be used incase the decision case is undefined. From above example, this could be [\"T1\"] Example { name : exclusive_join , taskReferenceName : exclusiveJoin , type : EXCLUSIVE_JOIN , joinOn : [ task2 , task3 ], defaultExclusiveJoinTask : [ task1 ] } Wait A wait task is implemented as a gate that remains in IN_PROGRESS state unless marked as COMPLETED or FAILED by an external trigger. To use a wait task, set the task type as WAIT Parameters: None required. External Triggers for Wait Task Task Resource endpoint can be used to update the status of a task to a terminate state. Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on. As the messages arrive, they are marked as COMPLETED or FAILED . SQS Queues SQS queues used by the server to update the task status can be retrieve using the following API: GET /queue When updating the status of the task, the message needs to conform to the following spec: Message has to be a valid JSON string. The message JSON should contain a key named externalId with the value being a JSONified string that contains the following keys: workflowId : Id of the workflow taskRefName : Task reference name that should be updated. Each queue represents a specific task status and tasks are marked accordingly. e.g. message coming to a COMPLETED queue marks the task status as COMPLETED . Tasks' output is updated with the message. Example SQS Payload: { some_key : valuex , externalId : {\\ taskRefName\\ :\\ TASK_REFERENCE_NAME\\ ,\\ workflowId\\ :\\ WORKFLOW_ID\\ } } Dynamic Task Dynamic Task allows to execute one of the registered Tasks dynamically at run-time. It accepts the task name to execute in inputParameters. Parameters: name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'. Example { name : user_task , taskReferenceName : t1 , inputParameters : { files : ${workflow.input.files} , taskToExecute : ${workflow.input.user_supplied_task} }, type : DYNAMIC , dynamicTaskNameParam : taskToExecute } If the workflow is started with input parameter user_supplied_task's value as user_task_2 , Conductor will schedule user_task_2 when scheduling this dynamic task. Lambda Task Lambda Task helps execute ad-hoc logic at Workflow run-time, using javax Nashorn Javascript evaluator engine. This is particularly helpful in running simple evaluations in Conductor server, over creating Workers. Parameters: name description Notes scriptExpression Javascript ( Nashorn ) evaluation expression defined as a string. Must return a value. Must be non-empty String. Example { name : LAMBDA_TASK , taskReferenceName : lambda_test , type : LAMBDA , inputParameters : { lambdaValue : ${workflow.input.lambdaValue} , scriptExpression : if ($.lambdaValue == 1){ return {testvalue: true} } else { return {testvalue: false} } } } The task output can then be referenced in downstream tasks like: \"${lambda_test.output.result.testvalue}\" Terminate Task Task that can terminate a workflow with a given status and modify the workflow's output with a given parameter. It can act as a \"return\" statement for conditions where you simply want to terminate your workflow. For example, if you have a decision where the first condition is met, you want to execute some tasks, otherwise you want to finish your workflow. Parameters: name description Notes terminationStatus can only accept \"COMPLETED\" or \"FAILED\" task cannot be optional workflowOutput Expected workflow output { name : terminate , taskReferenceName : terminate0 , inputParameters : { terminationStatus : COMPLETED , workflowOutput : ${task0.output} }, type : TERMINATE , startDelay : 0, optional : false } Kafka Publish Task A kafka Publish task is used to push messages to another microservice via kafka Parameters: The task expects an input parameter named kafka_request as part of the task's input with the following details: name description bootStrapServers bootStrapServers for connecting to given kafka. key Key to be published keySerializer Serializer used for serializing the key published to kafka. One of the following can be set : 1. org.apache.kafka.common.serialization.IntegerSerializer 2. org.apache.kafka.common.serialization.LongSerializer 3. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer value Value published to kafka requestTimeoutMs Request timeout while publishing to kafka. If this value is not given the value is read from the property kafka.publish.request.timeout.ms . If the property is not set the value defaults to 100 ms maxBlockMs maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms . If the property is not set the value defaults to 500 ms headers A map of additional kafka headers to be sent along with the request. topic Topic to publish The producer created in the kafka task is cached. By default the cache size is 10 and expiry time is 120000 ms. To change the defaults following can be modified kafka.publish.producer.cache.size,kafka.publish.producer.cache.time.ms respectively. Kafka Task Output Task status transitions to COMPLETED Example Task sample { name : call_kafka , taskReferenceName : call_kafka , inputParameters : { kafka_request : { topic : userTopic , value : Message to publish , bootStrapServers : localhost:9092 , headers : { x-Auth : Auth-key }, key : 123 , keySerializer : org.apache.kafka.common.serialization.IntegerSerializer } }, type : KAFKA_PUBLISH } The task is marked as FAILED if the message could not be published to the Kafka queue. Do While Task Do While Task allows tasks to be executed in loop until given condition become false. Condition is evaluated using nashorn javascript engine. Each iteration of loop over task will be scheduled as taskRefname__iteration. Iteration, any of loopover task's output or input parameters can be used to form a condition. Do while task output number of iterations with iteration as key and value as number of iterations. Each iteration's output will be stored as, iteration as key and loopover task's output as value Taskname which contains arithmetic operator must not be used in loopCondition. Any of loopOver task can be reference outside do while task same way other tasks are referenced. To reference specific iteration's output, $.LoopTask['iteration]['first_task'] Do while task does NOT support domain or isolation group execution. Nesting of DO_WHILE task is not supported. Loopover task must not be reused in neither workflow nor another DO_WHILE task. Parameters: name description loopCondition condition to be evaluated after every iteration loopOver List of tasks that needs to be executed in loop. Example { name : Loop Task , taskReferenceName : LoopTask , type : DO_WHILE , inputParameters : { value : ${workflow.input.value} }, loopCondition : if ( ($.LoopTask['iteration'] $.value ) || ( $.first_task['response']['body'] 10)) { false; } else { true; } , loopOver : [ { name : first_task , taskReferenceName : first_task , inputParameters : { http_request : { uri : http://localhost:8082 , method : POST } }, type : HTTP },{ name : second_task , taskReferenceName : second_task , inputParameters : { http_request : { uri : http://localhost:8082 , method : POST } }, type : HTTP } ], startDelay : 0, optional : false } If any of loopover task will be failed then do while task will be failed. In such case retry will start iteration from 1. TaskType SUB_WORKFLOW is not supported as a part of loopover task. Since loopover tasks will be executed in loop inside scope of parent do while task, crossing branching outside of DO_WHILE task will not be respected. Branching inside loopover task will be supported. In case of exception while evaluating loopCondition, do while task will be failed with FAILED_WITH_TERMINAL_ERROR. JSON JQ Transform Task JSON JQ Transform task allows transforming a JSON input to another JSON structure using a query expression. The input for the query ( . ) will be the inputParameters of the task. For JQ playground go to https://jqplay.org/ Parameters: name description queryExpression JQ query expression Example { name : jq_1 , taskReferenceName : jq_1 , type : JSON_JQ_TRANSFORM , inputParameters : { in1 : { arr : [ a , b ] }, in2 : { arr : [ c , d ] }, queryExpression : { out: (.in1.arr + .in2.arr) } } } In the example above the value of jq_1.output.result will be { \"out\": [\"a\",\"b\",\"c\",\"d\"] } The task output can then be referenced in downstream tasks like: \"${jq_1.output.result.out}\" Set Variable Task This task allows to set workflow variables by creating or updating them with new values. Variables can be initialized in the workflow definition as well as during the workflow run. Once a variable was initialized it can be read or overwritten with a new value by any other task. Warning There is a hard barrier for variables payload size in KB defined in the JVM system properties ( conductor.max.workflow.variables.payload.threshold.kb ) the default value is 256 . Passing this barrier will fail the task and the workflow. Parameters: The parameters for this task are the variable names with their respective values. Example { type : SET_VARIABLE , name : set_stage_start , taskReferenceName : set_stage_start , inputParameters : { stage : START } } Later in that workflow, the variable can be referenced by \"${workflow.variables.stage}\"","title":"System Tasks"},{"location":"configuration/systask/#decision","text":"A decision task is similar to case...switch statement in a programming language. The task takes 3 parameters: Parameters: name type description caseValueParam String Name of the parameter in task input whose value will be used as a switch. decisionCases Map[String, List[task]] Map where key is possible values of caseValueParam with value being list of tasks to be executed. defaultCase List[task] List of tasks to be executed when no matching value if found in decision case (default condition) caseExpression String Case expression to use instead of caseValueParam when the case should depend on complex values. This is a Javascript expression evaluated by the Nashorn Engine. Task names with arithmetic operators should not be used. Outputs: name type description caseOutput List[String] A List of string representing the list of cases that matched. Example { name : decide_task , taskReferenceName : decide1 , inputParameters : { case_value_param : ${workflow.input.movieType} }, type : DECISION , caseValueParam : case_value_param , decisionCases : { Show : [ { name : setup_episodes , taskReferenceName : se1 , inputParameters : { movieId : ${workflow.input.movieId} }, type : SIMPLE }, { name : generate_episode_artwork , taskReferenceName : ga , inputParameters : { movieId : ${workflow.input.movieId} }, type : SIMPLE } ], Movie : [ { name : setup_movie , taskReferenceName : sm , inputParameters : { movieId : ${workflow.input.movieId} }, type : SIMPLE }, { name : generate_movie_artwork , taskReferenceName : gma , inputParameters : { movieId : ${workflow.input.movieId} }, type : SIMPLE } ] } }","title":"Decision"},{"location":"configuration/systask/#event","text":"Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. Parameters: name type description sink String Qualified name of the event that is produced. e.g. conductor or sqs:sqs_queue_name asyncComplete Boolean false to mark status COMPLETED upon execution ; true to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. Outputs: name type description workflowInstanceId String Workflow id workflowType String Workflow Name workflowVersion Integer Workflow Version correlationId String Workflow CorrelationId sink String Copy of the input data \"sink\" asyncComplete Boolean Copy of the input data \"asyncComplete event_produced String Name of the event produced The published event's payload is identical to the output of the task (except \"event_produced\"). Example { sink : sqs:example_sqs_queue_name , asyncComplete : false } When producing an event with Conductor as sink, the event name follows the structure: conductor: workflow_name : task_reference_name For SQS, use the name of the queue and NOT the URI. Conductor looks up the URI based on the name. Warning When using SQS add the ContribsModule to the deployment. The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs. Supported Sinks Conductor SQS","title":"Event"},{"location":"configuration/systask/#http","text":"An HTTP task is used to make calls to another microservice over HTTP. Parameters: name type description http_request HttpRequest JSON object (see below) HttpRequest JSON object: name type description uri String URI for the service. Can be a partial when using vipAddress or includes the server address. method String HTTP method. One of the GET, PUT, POST, DELETE, OPTIONS, HEAD accept String Accept header as required by server. Defaults to application/json contentType String Content Type - supported types are text/plain , text/html , and application/json (Default) headers Map[String, Any] A map of additional http headers to be sent along with the request. body Map[] Request body vipAddress String When using discovery based service URLs. asyncComplete Boolean false to mark status COMPLETED upon execution ; true to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. oauthConsumerKey String OAuth client consumer key oauthConsumerSecret String OAuth client consumer secret connectionTimeOut Integer Connection Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 100. readTimeOut Integer Read Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 150. Output: name type description response Map JSON body containing the response if one is present headers Map[String, Any] Response Headers statusCode Integer Http Status Code reasonPhrase String Http Status Code's reason phrase Example Task Input payload using vipAddress { http_request : { vipAddress : examplevip-prod , uri : / , method : GET , accept : text/plain } } Task Input using an absolute URL { http_request : { uri : http://example.com/ , method : GET , accept : text/plain } } The task is marked as FAILED if the request cannot be completed or the remote server returns non successful status code. Note HTTP task currently only supports Content-Type as application/json and is able to parse the text as well as JSON response. XML input/output is currently not supported. However, if the response cannot be parsed as JSON or Text, a string representation is stored as a text value.","title":"HTTP"},{"location":"configuration/systask/#sub-workflow","text":"Sub Workflow task allows for nesting a workflow within another workflow. Parameters: name type description subWorkflowParam Map[String, Any] See below subWorkflowParam name type description name String Name of the workflow to execute version Integer Version of the workflow to execute taskToDomain Map[String, String] Allows scheduling the sub workflow's tasks per given mappings. See Task Domains for instructions to configure taskDomains. workflowDefinition WorkflowDefinition Allows starting a subworkflow with a dynamic workflow definition. Outputs: name type description subWorkflowId String Subworkflow execution Id generated when running the subworkflow Example { name : sub_workflow_task , taskReferenceName : sub1 , type : SUB_WORKFLOW , inputParameters : { subWorkflowParam : { name : deployment_workflow , version : 1, taskToDomain : { * : mydomain }, workflowDefinition : { name : deployment_workflow , description : Deploys to CDN , version : 1, tasks : [{ name : deploy , taskReferenceName : d1 , type : SIMPLE , inputParameters : { fileLocation : ${workflow.input.encodeLocation} } }], outputParameters : { cdn_url : ${d1.output.location} }, failureWorkflow : cleanup_encode_resources , restartable : true, workflowStatusListenerEnabled : true, schemaVersion : 2 } }, anythingelse : value } } When executed, a deployment_workflow is executed with its inputs parameters set to the inputParameters of the sub_workflow_task and the workflow definition specified. The task is marked as completed upon the completion of the spawned workflow. If the sub-workflow is terminated or fails the task is marked as failure and retried if configured.","title":"Sub Workflow"},{"location":"configuration/systask/#fork","text":"Fork is used to schedule parallel set of tasks, specified by \"type\":\"FORK_JOIN\" . Parameters: name description forkTasks A list of list of tasks. Each sublist is scheduled to be executed in parallel. However, tasks within the sublists are scheduled in a serial fashion. Example [ { name : fork_join , taskReferenceName : forkx , type : FORK_JOIN , forkTasks : [ [ { name : task_10 , taskReferenceName : task_A , type : SIMPLE }, { name : task_11 , taskReferenceName : task_B , type : SIMPLE } ], [ { name : task_21 , taskReferenceName : task_Y , type : SIMPLE }, { name : task_22 , taskReferenceName : task_Z , type : SIMPLE } ] ] }, { name : join , taskReferenceName : join2 , type : JOIN , joinOn : [ task_B , task_Z ] } ] When executed, task_A and task_Y are scheduled to be executed at the same time. Fork and Join A Join task MUST follow FORK_JOIN Workflow definition MUST include a Join task definition followed by FORK_JOIN task. Forked task can be a Sub Workflow, allowing for more complex execution flows.","title":"Fork"},{"location":"configuration/systask/#dynamic-fork","text":"A dynamic fork is same as FORK_JOIN task. Except that the list of tasks to be forked is provided at runtime using task's input. Useful when number of tasks to be forked is not fixed and varies based on the input. name description dynamicForkTasksParam Name of the parameter that contains list of workflow task configuration to be executed in parallel dynamicForkTasksInputParamName Name of the parameter whose value should be a map with key as forked task's reference name and value as input the forked task Example { inputParameters : { dynamicTasks : ${taskA.output.dynamicTasksJSON} , dynamicTasksInput : ${taskA.output.dynamicTasksInputJSON} }, type : FORK_JOIN_DYNAMIC , dynamicForkTasksParam : dynamicTasks , dynamicForkTasksInputParamName : dynamicTasksInput } Consider taskA 's output as: { dynamicTasksInputJSON : { forkedTask1 : { width : 100, height : 100, params : { recipe : jpg } }, forkedTask2 : { width : 200, height : 200, params : { recipe : jpg } } }, dynamicTasksJSON : [ { name : encode_task , taskReferenceName : forkedTask1 , type : SIMPLE }, { name : encode_task , taskReferenceName : forkedTask2 , type : SIMPLE } ] } When executed, the dynamic fork task will schedule two parallel task of type \"encode_task\" with reference names \"forkedTask1\" and \"forkedTask2\" and inputs as specified by _ dynamicTasksInputJSON_ Dynamic Fork and Join A Join task MUST follow FORK_JOIN_DYNAMIC Workflow definition MUST include a Join task definition followed by FORK_JOIN_DYNAMIC task. However, given the dynamic nature of the task, no joinOn parameters are required for this Join. The join will wait for ALL the forked branches to complete before completing. Unlike FORK, which can execute parallel flows with each fork executing a series of tasks in sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork. However, forked task can be a Sub Workflow, allowing for more complex execution flows.","title":"Dynamic Fork"},{"location":"configuration/systask/#join","text":"Join task is used to wait for completion of one or more tasks spawned by fork tasks. Parameters: name description joinOn List of task reference name, for which the JOIN will wait for completion. Example { joinOn : [ taskRef1 , taskRef3 ] } Join Task Output Fork task's output will be a JSON object with key being the task reference name and value as the output of the fork task.","title":"Join"},{"location":"configuration/systask/#exclusive-join","text":"Exclusive Join task helps capture Task output from Decision Task's flow. For example, If we have a Workflow with T1 - [Decision: T2/T3] - EJ, then based on the decision, Exclusive Join (EJ) will produce the output from T2 or T3. I.e What ever is the output of one of T2/T3 will be available to downstream tasks through Exclusive Join task. If Decision Task takes True/False as cases, then: True: T1 - T2 - EJ; EJ will have T2's output. False: T1 - T3 - EJ; EJ will have T3's output. Undefined: T1 - EJ; EJ will have T1's output. Parameters: name description joinOn List of task reference names, which the EXCLUSIVE_JOIN will lookout for to capture output. From above example, this could be [\"T2\", \"T3\"] defaultExclusiveJoinTask Task reference name, whose output should be used incase the decision case is undefined. From above example, this could be [\"T1\"] Example { name : exclusive_join , taskReferenceName : exclusiveJoin , type : EXCLUSIVE_JOIN , joinOn : [ task2 , task3 ], defaultExclusiveJoinTask : [ task1 ] }","title":"Exclusive Join"},{"location":"configuration/systask/#wait","text":"A wait task is implemented as a gate that remains in IN_PROGRESS state unless marked as COMPLETED or FAILED by an external trigger. To use a wait task, set the task type as WAIT Parameters: None required. External Triggers for Wait Task Task Resource endpoint can be used to update the status of a task to a terminate state. Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on. As the messages arrive, they are marked as COMPLETED or FAILED . SQS Queues SQS queues used by the server to update the task status can be retrieve using the following API: GET /queue When updating the status of the task, the message needs to conform to the following spec: Message has to be a valid JSON string. The message JSON should contain a key named externalId with the value being a JSONified string that contains the following keys: workflowId : Id of the workflow taskRefName : Task reference name that should be updated. Each queue represents a specific task status and tasks are marked accordingly. e.g. message coming to a COMPLETED queue marks the task status as COMPLETED . Tasks' output is updated with the message. Example SQS Payload: { some_key : valuex , externalId : {\\ taskRefName\\ :\\ TASK_REFERENCE_NAME\\ ,\\ workflowId\\ :\\ WORKFLOW_ID\\ } }","title":"Wait"},{"location":"configuration/systask/#dynamic-task","text":"Dynamic Task allows to execute one of the registered Tasks dynamically at run-time. It accepts the task name to execute in inputParameters. Parameters: name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'. Example { name : user_task , taskReferenceName : t1 , inputParameters : { files : ${workflow.input.files} , taskToExecute : ${workflow.input.user_supplied_task} }, type : DYNAMIC , dynamicTaskNameParam : taskToExecute } If the workflow is started with input parameter user_supplied_task's value as user_task_2 , Conductor will schedule user_task_2 when scheduling this dynamic task.","title":"Dynamic Task"},{"location":"configuration/systask/#lambda-task","text":"Lambda Task helps execute ad-hoc logic at Workflow run-time, using javax Nashorn Javascript evaluator engine. This is particularly helpful in running simple evaluations in Conductor server, over creating Workers. Parameters: name description Notes scriptExpression Javascript ( Nashorn ) evaluation expression defined as a string. Must return a value. Must be non-empty String. Example { name : LAMBDA_TASK , taskReferenceName : lambda_test , type : LAMBDA , inputParameters : { lambdaValue : ${workflow.input.lambdaValue} , scriptExpression : if ($.lambdaValue == 1){ return {testvalue: true} } else { return {testvalue: false} } } } The task output can then be referenced in downstream tasks like: \"${lambda_test.output.result.testvalue}\"","title":"Lambda Task"},{"location":"configuration/systask/#terminate-task","text":"Task that can terminate a workflow with a given status and modify the workflow's output with a given parameter. It can act as a \"return\" statement for conditions where you simply want to terminate your workflow. For example, if you have a decision where the first condition is met, you want to execute some tasks, otherwise you want to finish your workflow. Parameters: name description Notes terminationStatus can only accept \"COMPLETED\" or \"FAILED\" task cannot be optional workflowOutput Expected workflow output { name : terminate , taskReferenceName : terminate0 , inputParameters : { terminationStatus : COMPLETED , workflowOutput : ${task0.output} }, type : TERMINATE , startDelay : 0, optional : false }","title":"Terminate Task"},{"location":"configuration/systask/#kafka-publish-task","text":"A kafka Publish task is used to push messages to another microservice via kafka Parameters: The task expects an input parameter named kafka_request as part of the task's input with the following details: name description bootStrapServers bootStrapServers for connecting to given kafka. key Key to be published keySerializer Serializer used for serializing the key published to kafka. One of the following can be set : 1. org.apache.kafka.common.serialization.IntegerSerializer 2. org.apache.kafka.common.serialization.LongSerializer 3. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer value Value published to kafka requestTimeoutMs Request timeout while publishing to kafka. If this value is not given the value is read from the property kafka.publish.request.timeout.ms . If the property is not set the value defaults to 100 ms maxBlockMs maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms . If the property is not set the value defaults to 500 ms headers A map of additional kafka headers to be sent along with the request. topic Topic to publish The producer created in the kafka task is cached. By default the cache size is 10 and expiry time is 120000 ms. To change the defaults following can be modified kafka.publish.producer.cache.size,kafka.publish.producer.cache.time.ms respectively. Kafka Task Output Task status transitions to COMPLETED Example Task sample { name : call_kafka , taskReferenceName : call_kafka , inputParameters : { kafka_request : { topic : userTopic , value : Message to publish , bootStrapServers : localhost:9092 , headers : { x-Auth : Auth-key }, key : 123 , keySerializer : org.apache.kafka.common.serialization.IntegerSerializer } }, type : KAFKA_PUBLISH } The task is marked as FAILED if the message could not be published to the Kafka queue.","title":"Kafka Publish Task"},{"location":"configuration/systask/#do-while-task","text":"Do While Task allows tasks to be executed in loop until given condition become false. Condition is evaluated using nashorn javascript engine. Each iteration of loop over task will be scheduled as taskRefname__iteration. Iteration, any of loopover task's output or input parameters can be used to form a condition. Do while task output number of iterations with iteration as key and value as number of iterations. Each iteration's output will be stored as, iteration as key and loopover task's output as value Taskname which contains arithmetic operator must not be used in loopCondition. Any of loopOver task can be reference outside do while task same way other tasks are referenced. To reference specific iteration's output, $.LoopTask['iteration]['first_task'] Do while task does NOT support domain or isolation group execution. Nesting of DO_WHILE task is not supported. Loopover task must not be reused in neither workflow nor another DO_WHILE task. Parameters: name description loopCondition condition to be evaluated after every iteration loopOver List of tasks that needs to be executed in loop. Example { name : Loop Task , taskReferenceName : LoopTask , type : DO_WHILE , inputParameters : { value : ${workflow.input.value} }, loopCondition : if ( ($.LoopTask['iteration'] $.value ) || ( $.first_task['response']['body'] 10)) { false; } else { true; } , loopOver : [ { name : first_task , taskReferenceName : first_task , inputParameters : { http_request : { uri : http://localhost:8082 , method : POST } }, type : HTTP },{ name : second_task , taskReferenceName : second_task , inputParameters : { http_request : { uri : http://localhost:8082 , method : POST } }, type : HTTP } ], startDelay : 0, optional : false } If any of loopover task will be failed then do while task will be failed. In such case retry will start iteration from 1. TaskType SUB_WORKFLOW is not supported as a part of loopover task. Since loopover tasks will be executed in loop inside scope of parent do while task, crossing branching outside of DO_WHILE task will not be respected. Branching inside loopover task will be supported. In case of exception while evaluating loopCondition, do while task will be failed with FAILED_WITH_TERMINAL_ERROR.","title":"Do While Task"},{"location":"configuration/systask/#json-jq-transform-task","text":"JSON JQ Transform task allows transforming a JSON input to another JSON structure using a query expression. The input for the query ( . ) will be the inputParameters of the task. For JQ playground go to https://jqplay.org/ Parameters: name description queryExpression JQ query expression Example { name : jq_1 , taskReferenceName : jq_1 , type : JSON_JQ_TRANSFORM , inputParameters : { in1 : { arr : [ a , b ] }, in2 : { arr : [ c , d ] }, queryExpression : { out: (.in1.arr + .in2.arr) } } } In the example above the value of jq_1.output.result will be { \"out\": [\"a\",\"b\",\"c\",\"d\"] } The task output can then be referenced in downstream tasks like: \"${jq_1.output.result.out}\"","title":"JSON JQ Transform Task"},{"location":"configuration/systask/#set-variable-task","text":"This task allows to set workflow variables by creating or updating them with new values. Variables can be initialized in the workflow definition as well as during the workflow run. Once a variable was initialized it can be read or overwritten with a new value by any other task. Warning There is a hard barrier for variables payload size in KB defined in the JVM system properties ( conductor.max.workflow.variables.payload.threshold.kb ) the default value is 256 . Passing this barrier will fail the task and the workflow. Parameters: The parameters for this task are the variable names with their respective values. Example { type : SET_VARIABLE , name : set_stage_start , taskReferenceName : set_stage_start , inputParameters : { stage : START } } Later in that workflow, the variable can be referenced by \"${workflow.variables.stage}\"","title":"Set Variable Task"},{"location":"configuration/taskdef/","text":"Task Definition Conductor maintains a registry of worker tasks. A task MUST be registered before being used in a workflow. Example { name : encode_task , retryCount : 3, timeoutSeconds : 1200, pollTimeoutSeconds : 3600, inputKeys : [ sourceRequestId , qcElementType ], outputKeys : [ state , skipped , result ], timeoutPolicy : TIME_OUT_WF , retryLogic : FIXED , retryDelaySeconds : 600, responseTimeoutSeconds : 3600, concurrentExecLimit : 100, rateLimitFrequencyInSeconds : 60, rateLimitPerFrequency : 50 } field description Notes name Task Type. Unique name of the Task that resonates with it's function. Unique description Description of the task optional retryCount No. of retries to attempt when a Task is marked as failure defaults to 3 retryLogic Mechanism for the retries see possible values below retryDelaySeconds Time to wait before retries defaults to 60 seconds timeoutPolicy Task's timeout policy see possible values below timeoutSeconds Time in seconds, after which the task is marked as TIMED_OUT if not completed after transitioning to IN_PROGRESS status for the first time No timeouts if set to 0 pollTimeoutSeconds Time in seconds, after which the task is marked as TIMED_OUT if not polled by a worker No timeouts if set to 0 responseTimeoutSeconds Must be greater than 0 and less than timeoutSeconds. The task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. defaults to 3600 inputKeys Array of keys of task's expected input. Used for documenting task's input. See Using inputKeys and outputKeys . optional outputKeys Array of keys of task's expected output. Used for documenting task's output optional inputTemplate See Using inputTemplate below. optional concurrentExecLimit Number of tasks that can be executed at any given time. optional rateLimitFrequencyInSeconds, rateLimitPerFrequency See Task Rate limits below. optional Retry Logic FIXED : Reschedule the task after the retryDelaySeconds EXPONENTIAL_BACKOFF : Reschedule after retryDelaySeconds * attemptNumber Timeout Policy RETRY : Retries the task again TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated ALERT_ONLY : Registers a counter (task_timeout) Task Concurrent Execution Limits concurrentExecLimit limits the number of simultaneous Task executions at any point. Example: If you have 1000 task executions waiting in the queue, and 1000 workers polling this queue for tasks, but if you have set concurrentExecLimit to 10, only 10 tasks would be given to workers (which would lead to starvation). If any of the workers finishes execution, a new task(s) will be removed from the queue, while still keeping the current execution count to 10. Task Rate limits rateLimitFrequencyInSeconds and rateLimitPerFrequency should be used together. rateLimitFrequencyInSeconds sets the \"frequency window\", i.e the duration to be used in events per duration . Eg: 1s, 5s, 60s, 300s etc. rateLimitPerFrequency defines the number of Tasks that can be given to Workers per given \"frequency window\". Example: Let's set rateLimitFrequencyInSeconds = 5 , and rateLimitPerFrequency = 12 . This means, our frequency window is of 5 seconds duration, and for each frequency window, Conductor would only give 12 tasks to workers. So, in a given minute, Conductor would only give 12*(60/5) = 144 tasks to workers irrespective of the number of workers that are polling for the task. Note that unlike concurrentExecLimit , rate limiting doesn't take into account tasks already in progress/completed. Even if all the previous tasks are executed within 1 sec, or would take a few days, the new tasks are still given to workers at configured frequency, 144 tasks per minute in above example. Note: Rate limiting is only supported for the Redis-persistence module and is not available with other persistence layers. Using inputKeys and outputKeys inputKeys and outputKeys can be considered as parameters and return values for the Task. Consider the task Definition as being represented by an interface: (value1, value2 .. valueN) someTaskDefinition(key1, key2 .. keyN); However, these parameters are not strictly enforced at the moment. Both inputKeys and outputKeys act as a documentation for task re-use. The tasks in workflow need not define all of the keys in the task definition. In the future, this can be extended to be a strict template that all task implementations must adhere to, just like interfaces in programming languages. Using inputTemplate inputTemplate allows to define default values, which can be overridden by values provided in Workflow. Eg: In your Task Definition, you can define your inputTemplate as: inputTemplate : { url : https://some_url:7004 } Now, in your workflow Definition, when using above task, you can use the default url or override with something else in the task's inputParameters . inputParameters : { url : ${workflow.input.some_new_url} }","title":"Task Definition"},{"location":"configuration/taskdef/#task-definition","text":"Conductor maintains a registry of worker tasks. A task MUST be registered before being used in a workflow. Example { name : encode_task , retryCount : 3, timeoutSeconds : 1200, pollTimeoutSeconds : 3600, inputKeys : [ sourceRequestId , qcElementType ], outputKeys : [ state , skipped , result ], timeoutPolicy : TIME_OUT_WF , retryLogic : FIXED , retryDelaySeconds : 600, responseTimeoutSeconds : 3600, concurrentExecLimit : 100, rateLimitFrequencyInSeconds : 60, rateLimitPerFrequency : 50 } field description Notes name Task Type. Unique name of the Task that resonates with it's function. Unique description Description of the task optional retryCount No. of retries to attempt when a Task is marked as failure defaults to 3 retryLogic Mechanism for the retries see possible values below retryDelaySeconds Time to wait before retries defaults to 60 seconds timeoutPolicy Task's timeout policy see possible values below timeoutSeconds Time in seconds, after which the task is marked as TIMED_OUT if not completed after transitioning to IN_PROGRESS status for the first time No timeouts if set to 0 pollTimeoutSeconds Time in seconds, after which the task is marked as TIMED_OUT if not polled by a worker No timeouts if set to 0 responseTimeoutSeconds Must be greater than 0 and less than timeoutSeconds. The task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. defaults to 3600 inputKeys Array of keys of task's expected input. Used for documenting task's input. See Using inputKeys and outputKeys . optional outputKeys Array of keys of task's expected output. Used for documenting task's output optional inputTemplate See Using inputTemplate below. optional concurrentExecLimit Number of tasks that can be executed at any given time. optional rateLimitFrequencyInSeconds, rateLimitPerFrequency See Task Rate limits below. optional","title":"Task Definition"},{"location":"configuration/taskdef/#retry-logic","text":"FIXED : Reschedule the task after the retryDelaySeconds EXPONENTIAL_BACKOFF : Reschedule after retryDelaySeconds * attemptNumber","title":"Retry Logic"},{"location":"configuration/taskdef/#timeout-policy","text":"RETRY : Retries the task again TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated ALERT_ONLY : Registers a counter (task_timeout)","title":"Timeout Policy"},{"location":"configuration/taskdef/#task-concurrent-execution-limits","text":"concurrentExecLimit limits the number of simultaneous Task executions at any point. Example: If you have 1000 task executions waiting in the queue, and 1000 workers polling this queue for tasks, but if you have set concurrentExecLimit to 10, only 10 tasks would be given to workers (which would lead to starvation). If any of the workers finishes execution, a new task(s) will be removed from the queue, while still keeping the current execution count to 10.","title":"Task Concurrent Execution Limits"},{"location":"configuration/taskdef/#task-rate-limits","text":"rateLimitFrequencyInSeconds and rateLimitPerFrequency should be used together. rateLimitFrequencyInSeconds sets the \"frequency window\", i.e the duration to be used in events per duration . Eg: 1s, 5s, 60s, 300s etc. rateLimitPerFrequency defines the number of Tasks that can be given to Workers per given \"frequency window\". Example: Let's set rateLimitFrequencyInSeconds = 5 , and rateLimitPerFrequency = 12 . This means, our frequency window is of 5 seconds duration, and for each frequency window, Conductor would only give 12 tasks to workers. So, in a given minute, Conductor would only give 12*(60/5) = 144 tasks to workers irrespective of the number of workers that are polling for the task. Note that unlike concurrentExecLimit , rate limiting doesn't take into account tasks already in progress/completed. Even if all the previous tasks are executed within 1 sec, or would take a few days, the new tasks are still given to workers at configured frequency, 144 tasks per minute in above example. Note: Rate limiting is only supported for the Redis-persistence module and is not available with other persistence layers.","title":"Task Rate limits"},{"location":"configuration/taskdef/#using-inputkeys-and-outputkeys","text":"inputKeys and outputKeys can be considered as parameters and return values for the Task. Consider the task Definition as being represented by an interface: (value1, value2 .. valueN) someTaskDefinition(key1, key2 .. keyN); However, these parameters are not strictly enforced at the moment. Both inputKeys and outputKeys act as a documentation for task re-use. The tasks in workflow need not define all of the keys in the task definition. In the future, this can be extended to be a strict template that all task implementations must adhere to, just like interfaces in programming languages.","title":"Using inputKeys and outputKeys"},{"location":"configuration/taskdef/#using-inputtemplate","text":"inputTemplate allows to define default values, which can be overridden by values provided in Workflow. Eg: In your Task Definition, you can define your inputTemplate as: inputTemplate : { url : https://some_url:7004 } Now, in your workflow Definition, when using above task, you can use the default url or override with something else in the task's inputParameters . inputParameters : { url : ${workflow.input.some_new_url} }","title":"Using inputTemplate"},{"location":"configuration/taskdomains/","text":"Task Domains Task domains helps support task development. The idea is same \u201ctask definition\u201d can be implemented in different \u201cdomains\u201d. A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it. As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \u201cTask Domain\u201d feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task. When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on. If no workers are active for the domains provided: If NO_DOMAIN is provided as last token in list of domains, then no domain is set. Else, task will be added to last inactive domain in list of domains, hoping that workers would soon be available for that domain. Also, a * token can be used to apply domains for all tasks. This can be overridden by providing task specific mappings along with * . For example, the below configuration: taskToDomain : { * : mydomain , some_task_x : NO_DOMAIN , some_task_y : someDomain, NO_DOMAIN , some_task_z : someInactiveDomain1, someInactiveDomain2 } puts some_task_x in default queue (no domain). puts some_task_y in someDomain domain, if available or in default otherwise. puts some_task_z in someInactiveDomain2 , even though workers are not available yet. and puts all other tasks in mydomain (even if workers are not available). Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. Also, NO_DOMAIN token should be used last. How to use Task Domains Change the poll call The poll call must now specify the domain. Java Client If you are using the java client then a simple property change will force TaskRunnerConfigurer to pass the domain to the poller. conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain mydomain REST call GET /tasks/poll/batch/T2?workerid=myworker domain=mydomain GET /tasks/poll/T2?workerid=myworker domain=mydomain Change the start workflow call When starting the workflow, make sure the task to domain mapping is passes Java Client Map String, Object input = new HashMap (); input.put( wf_input1 , one\u201d); Map String, String taskToDomain = new HashMap (); taskToDomain.put( T2 , mydomain ); // Other options ... // taskToDomain.put( * , mydomain, NO_DOMAIN ) // taskToDomain.put( T2 , mydomain, fallbackDomain1, fallbackDomain2 ) StartWorkflowRequest swr = new StartWorkflowRequest(); swr.withName(\u201cmyWorkflow\u201d) .withCorrelationId(\u201ccorr1\u201d) .withVersion(1) .withInput(input) .withTaskToDomain(taskToDomain); wfclient.startWorkflow(swr); REST call POST /workflow { name : myWorkflow , version : 1, correlatonId : corr1 input : { wf_input1 : one }, taskToDomain : { * : mydomain , some_task_x : NO_DOMAIN , some_task_y : someDomain, NO_DOMAIN } }","title":"Task Domains"},{"location":"configuration/taskdomains/#task-domains","text":"Task domains helps support task development. The idea is same \u201ctask definition\u201d can be implemented in different \u201cdomains\u201d. A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it. As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \u201cTask Domain\u201d feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task. When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on. If no workers are active for the domains provided: If NO_DOMAIN is provided as last token in list of domains, then no domain is set. Else, task will be added to last inactive domain in list of domains, hoping that workers would soon be available for that domain. Also, a * token can be used to apply domains for all tasks. This can be overridden by providing task specific mappings along with * . For example, the below configuration: taskToDomain : { * : mydomain , some_task_x : NO_DOMAIN , some_task_y : someDomain, NO_DOMAIN , some_task_z : someInactiveDomain1, someInactiveDomain2 } puts some_task_x in default queue (no domain). puts some_task_y in someDomain domain, if available or in default otherwise. puts some_task_z in someInactiveDomain2 , even though workers are not available yet. and puts all other tasks in mydomain (even if workers are not available). Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. Also, NO_DOMAIN token should be used last.","title":"Task Domains"},{"location":"configuration/taskdomains/#how-to-use-task-domains","text":"","title":"How to use Task Domains"},{"location":"configuration/taskdomains/#change-the-poll-call","text":"The poll call must now specify the domain.","title":"Change the poll call"},{"location":"configuration/taskdomains/#java-client","text":"If you are using the java client then a simple property change will force TaskRunnerConfigurer to pass the domain to the poller. conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain mydomain","title":"Java Client"},{"location":"configuration/taskdomains/#rest-call","text":"GET /tasks/poll/batch/T2?workerid=myworker domain=mydomain GET /tasks/poll/T2?workerid=myworker domain=mydomain","title":"REST call"},{"location":"configuration/taskdomains/#change-the-start-workflow-call","text":"When starting the workflow, make sure the task to domain mapping is passes","title":"Change the start workflow call"},{"location":"configuration/taskdomains/#java-client_1","text":"Map String, Object input = new HashMap (); input.put( wf_input1 , one\u201d); Map String, String taskToDomain = new HashMap (); taskToDomain.put( T2 , mydomain ); // Other options ... // taskToDomain.put( * , mydomain, NO_DOMAIN ) // taskToDomain.put( T2 , mydomain, fallbackDomain1, fallbackDomain2 ) StartWorkflowRequest swr = new StartWorkflowRequest(); swr.withName(\u201cmyWorkflow\u201d) .withCorrelationId(\u201ccorr1\u201d) .withVersion(1) .withInput(input) .withTaskToDomain(taskToDomain); wfclient.startWorkflow(swr);","title":"Java Client"},{"location":"configuration/taskdomains/#rest-call_1","text":"POST /workflow { name : myWorkflow , version : 1, correlatonId : corr1 input : { wf_input1 : one }, taskToDomain : { * : mydomain , some_task_x : NO_DOMAIN , some_task_y : someDomain, NO_DOMAIN } }","title":"REST call"},{"location":"configuration/workflowdef/","text":"Workflow Definition Workflows are defined using a JSON based DSL. Example { name : encode_and_deploy , description : Encodes a file and deploys to CDN , version : 1, tasks : [ { name : encode , taskReferenceName : encode , type : SIMPLE , inputParameters : { fileLocation : ${workflow.input.fileLocation} } }, { name : deploy , taskReferenceName : d1 , type : SIMPLE , inputParameters : { fileLocation : ${encode.output.encodeLocation} } } ], outputParameters : { cdn_url : ${d1.output.location} }, failureWorkflow : cleanup_encode_resources , restartable : true, workflowStatusListenerEnabled : true, schemaVersion : 2 } field description Notes name Name of the workflow description Description of the workflow optional version Numeric field used to identify the version of the schema. Use incrementing numbers When starting a workflow execution, if not specified, the definition with highest version is used tasks An array of task definitions as described below. inputParameters List of input parameters. Used for documenting the required inputs to workflow optional outputParameters JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task failureWorkflow String; Workflow to be run on current Workflow failure. Useful for cleanup or post actions on failure. optional schemaVersion Current Conductor Schema version. schemaVersion 1 is discontinued. Must be 2 restartable Boolean flag to allow Workflow restarts defaults to true workflowStatusListenerEnabled If true, every workflow that gets terminated or completed will send a notification. See below optional (false by default) Tasks within Workflow tasks property in a workflow execution defines an array of tasks to be executed in that order. field description Notes name Name of the task. MUST be registered as a task with Conductor before starting the workflow taskReferenceName Alias used to refer the task within the workflow. MUST be unique within workflow. type Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types description Description of the task optional optional true or false. When set to true - workflow continues even if the task fails. The status of the task is reflected as COMPLETED_WITH_ERRORS Defaults to false inputParameters JSON template that defines the input given to the task See Wiring Inputs and Outputs for details domain See Task Domains for more information. optional In addition to these parameters, System Tasks have their own parameters. Checkout System Tasks for more information. Wiring Inputs and Outputs Workflows are supplied inputs by client when a new execution is triggered. Workflow input is a JSON payload that is available via ${workflow.input...} expressions. Each task in the workflow is given input based on the inputParameters template configured in workflow definition. inputParameters is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution. Syntax for mapping the values follows the pattern as: ${SOURCE.input/output.JSONPath} field description SOURCE can be either \"workflow\" or any of the task reference name input/output refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output JSON Path Support Conductor supports JSONPath specification and uses Java implementation from here . Example Consider a task with input configured to use input/output parameters from workflow and a task named loc_task . { inputParameters : { movieId : ${workflow.input.movieId} , url : ${workflow.input.fileLocation} , lang : ${loc_task.output.languages[0]} , http_request : { method : POST , url : http://example.com/${loc_task.output.fileId}/encode , body : { recipe : ${workflow.input.recipe} , params : { width : 100, height : 100 } }, headers : { Accept : application/json , Content-Type : application/json } } } } Consider the following as the workflow input { movieId : movie_123 , fileLocation : s3://moviebucket/file123 , recipe : png } And the output of the loc_task as the following; { fileId : file_xxx_yyy_zzz , languages : [ en , ja , es ] } When scheduling the task, Conductor will merge the values from workflow input and loc_task's output and create the input to the task as follows: { movieId : movie_123 , url : s3://moviebucket/file123 , lang : en , http_request : { method : POST , url : http://example.com/file_xxx_yyy_zzz/encode , body : { recipe : png , params : { width : 100, height : 100 } }, headers : { Accept : application/json , Content-Type : application/json } } } Workflow notifications Conductor can be configured to publish notifications to external systems upon completion/termination of workflows. See extending conductor for details.","title":"Workflow Definition"},{"location":"configuration/workflowdef/#workflow-definition","text":"Workflows are defined using a JSON based DSL. Example { name : encode_and_deploy , description : Encodes a file and deploys to CDN , version : 1, tasks : [ { name : encode , taskReferenceName : encode , type : SIMPLE , inputParameters : { fileLocation : ${workflow.input.fileLocation} } }, { name : deploy , taskReferenceName : d1 , type : SIMPLE , inputParameters : { fileLocation : ${encode.output.encodeLocation} } } ], outputParameters : { cdn_url : ${d1.output.location} }, failureWorkflow : cleanup_encode_resources , restartable : true, workflowStatusListenerEnabled : true, schemaVersion : 2 } field description Notes name Name of the workflow description Description of the workflow optional version Numeric field used to identify the version of the schema. Use incrementing numbers When starting a workflow execution, if not specified, the definition with highest version is used tasks An array of task definitions as described below. inputParameters List of input parameters. Used for documenting the required inputs to workflow optional outputParameters JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task failureWorkflow String; Workflow to be run on current Workflow failure. Useful for cleanup or post actions on failure. optional schemaVersion Current Conductor Schema version. schemaVersion 1 is discontinued. Must be 2 restartable Boolean flag to allow Workflow restarts defaults to true workflowStatusListenerEnabled If true, every workflow that gets terminated or completed will send a notification. See below optional (false by default)","title":"Workflow Definition"},{"location":"configuration/workflowdef/#tasks-within-workflow","text":"tasks property in a workflow execution defines an array of tasks to be executed in that order. field description Notes name Name of the task. MUST be registered as a task with Conductor before starting the workflow taskReferenceName Alias used to refer the task within the workflow. MUST be unique within workflow. type Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types description Description of the task optional optional true or false. When set to true - workflow continues even if the task fails. The status of the task is reflected as COMPLETED_WITH_ERRORS Defaults to false inputParameters JSON template that defines the input given to the task See Wiring Inputs and Outputs for details domain See Task Domains for more information. optional In addition to these parameters, System Tasks have their own parameters. Checkout System Tasks for more information.","title":"Tasks within Workflow"},{"location":"configuration/workflowdef/#wiring-inputs-and-outputs","text":"Workflows are supplied inputs by client when a new execution is triggered. Workflow input is a JSON payload that is available via ${workflow.input...} expressions. Each task in the workflow is given input based on the inputParameters template configured in workflow definition. inputParameters is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution. Syntax for mapping the values follows the pattern as: ${SOURCE.input/output.JSONPath} field description SOURCE can be either \"workflow\" or any of the task reference name input/output refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output JSON Path Support Conductor supports JSONPath specification and uses Java implementation from here . Example Consider a task with input configured to use input/output parameters from workflow and a task named loc_task . { inputParameters : { movieId : ${workflow.input.movieId} , url : ${workflow.input.fileLocation} , lang : ${loc_task.output.languages[0]} , http_request : { method : POST , url : http://example.com/${loc_task.output.fileId}/encode , body : { recipe : ${workflow.input.recipe} , params : { width : 100, height : 100 } }, headers : { Accept : application/json , Content-Type : application/json } } } } Consider the following as the workflow input { movieId : movie_123 , fileLocation : s3://moviebucket/file123 , recipe : png } And the output of the loc_task as the following; { fileId : file_xxx_yyy_zzz , languages : [ en , ja , es ] } When scheduling the task, Conductor will merge the values from workflow input and loc_task's output and create the input to the task as follows: { movieId : movie_123 , url : s3://moviebucket/file123 , lang : en , http_request : { method : POST , url : http://example.com/file_xxx_yyy_zzz/encode , body : { recipe : png , params : { width : 100, height : 100 } }, headers : { Accept : application/json , Content-Type : application/json } } }","title":"Wiring Inputs and Outputs"},{"location":"configuration/workflowdef/#workflow-notifications","text":"Conductor can be configured to publish notifications to external systems upon completion/termination of workflows. See extending conductor for details.","title":"Workflow notifications"},{"location":"gettingstarted/basicconcepts/","text":"Definitions (aka Metadata or Blueprints) Conductor definitions are like class definitions in OOP paradigm, or templates. You define this once, and use for each workflow execution. Definitions to Executions have 1:N relationship. Tasks Tasks are the building blocks of Workflow. There must be at least one task in a Workflow. Tasks can be categorized into two types: Systems tasks - executed by Conductor server. Worker tasks - executed by your own workers. Workflow A Workflow is the container of your process flow. It could include several different types of Tasks, Sub-Workflows, inputs and outputs connected to each other, to effectively achieve the desired result. Workflow Definition Workflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows. The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. Detailed description Task Definition Task definitions help define Task level parameters like inputs and outputs, timeouts, retries etc. All tasks need to be registered before they can be used by active workflows. A task can be re-used within multiple workflows. Detailed description System Tasks System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability. See Systems tasks for list of available Task types, and instructions for using them. Note Conductor provides an API to create user defined tasks that are executed in the same JVM as the engine. See WorkflowSystemTask interface for details. Worker Tasks Worker tasks are implemented by your application(s) and run in a separate environment from Conductor. The worker tasks can be implemented in any language. These tasks talk to Conductor server via REST/gRPC to poll for tasks and update its status after execution. Worker tasks are identified by task type SIMPLE in the blueprint.","title":"Basic Concepts"},{"location":"gettingstarted/basicconcepts/#definitions-aka-metadata-or-blueprints","text":"Conductor definitions are like class definitions in OOP paradigm, or templates. You define this once, and use for each workflow execution. Definitions to Executions have 1:N relationship.","title":"Definitions (aka Metadata or Blueprints)"},{"location":"gettingstarted/basicconcepts/#tasks","text":"Tasks are the building blocks of Workflow. There must be at least one task in a Workflow. Tasks can be categorized into two types: Systems tasks - executed by Conductor server. Worker tasks - executed by your own workers.","title":"Tasks"},{"location":"gettingstarted/basicconcepts/#workflow","text":"A Workflow is the container of your process flow. It could include several different types of Tasks, Sub-Workflows, inputs and outputs connected to each other, to effectively achieve the desired result.","title":"Workflow"},{"location":"gettingstarted/basicconcepts/#workflow-definition","text":"Workflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows. The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. Detailed description","title":"Workflow Definition"},{"location":"gettingstarted/basicconcepts/#task-definition","text":"Task definitions help define Task level parameters like inputs and outputs, timeouts, retries etc. All tasks need to be registered before they can be used by active workflows. A task can be re-used within multiple workflows. Detailed description","title":"Task Definition"},{"location":"gettingstarted/basicconcepts/#system-tasks","text":"System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability. See Systems tasks for list of available Task types, and instructions for using them. Note Conductor provides an API to create user defined tasks that are executed in the same JVM as the engine. See WorkflowSystemTask interface for details.","title":"System Tasks"},{"location":"gettingstarted/basicconcepts/#worker-tasks","text":"Worker tasks are implemented by your application(s) and run in a separate environment from Conductor. The worker tasks can be implemented in any language. These tasks talk to Conductor server via REST/gRPC to poll for tasks and update its status after execution. Worker tasks are identified by task type SIMPLE in the blueprint.","title":"Worker Tasks"},{"location":"gettingstarted/client/","text":"Conductor tasks that are executed by remote workers communicate over HTTP endpoints/gRPC to poll for the task and update the status of the execution. Client APIs Conductor provides the following java clients to interact with the various APIs Client Usage Metadata Client Register / Update workflow and task definitions Workflow Client Start a new workflow / Get execution status of a workflow Task Client Poll for task / Update task result after execution / Get status of a task Java Worker Conductor provides an automated framework to poll for tasks, manage the execution thread and update the status of the execution back to the server. Implement the Worker interface to execute the task. TaskRunnerConfigurer The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop. Manages the task workers thread pool and server communication (poll and task update). Use the Builder to create an instance of the TaskRunnerConfigurer. The builder accepts the following parameters: Initialize the Builder with the following: TaskClient | TaskClient used to communicate to the Conductor server | | Workers | Workers that will be used for polling work and task execution. | Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- Once an instance is created, call init() method to initialize the TaskPollExecutor and begin the polling and execution of tasks. Note To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided shutdown() hook in a PreDestroy block. Properties The worker behavior can be further controlled by using these properties: Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM: Name Description conductor.worker. property Applies to ALL the workers in the JVM. conductor.worker. taskDefName . property Applies to the specified worker. Overrides the global property. Examples Sample Worker Implementation Example Python https://github.com/Netflix/conductor/tree/dev/client/python Follow the example as documented in the readme or take a look at kitchensink_workers.py Warning Python client is a community contribution. We encourage you to test it out and let us know the feedback. Pull Requests with fixes or enhancements are welcomed!","title":"Using the Client"},{"location":"gettingstarted/client/#client-apis","text":"Conductor provides the following java clients to interact with the various APIs Client Usage Metadata Client Register / Update workflow and task definitions Workflow Client Start a new workflow / Get execution status of a workflow Task Client Poll for task / Update task result after execution / Get status of a task","title":"Client APIs"},{"location":"gettingstarted/client/#java","text":"","title":"Java"},{"location":"gettingstarted/client/#worker","text":"Conductor provides an automated framework to poll for tasks, manage the execution thread and update the status of the execution back to the server. Implement the Worker interface to execute the task.","title":"Worker"},{"location":"gettingstarted/client/#taskrunnerconfigurer","text":"The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop. Manages the task workers thread pool and server communication (poll and task update). Use the Builder to create an instance of the TaskRunnerConfigurer. The builder accepts the following parameters: Initialize the Builder with the following: TaskClient | TaskClient used to communicate to the Conductor server | | Workers | Workers that will be used for polling work and task execution. | Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- Once an instance is created, call init() method to initialize the TaskPollExecutor and begin the polling and execution of tasks. Note To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided shutdown() hook in a PreDestroy block. Properties The worker behavior can be further controlled by using these properties: Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM: Name Description conductor.worker. property Applies to ALL the workers in the JVM. conductor.worker. taskDefName . property Applies to the specified worker. Overrides the global property. Examples Sample Worker Implementation Example","title":"TaskRunnerConfigurer"},{"location":"gettingstarted/client/#python","text":"https://github.com/Netflix/conductor/tree/dev/client/python Follow the example as documented in the readme or take a look at kitchensink_workers.py Warning Python client is a community contribution. We encourage you to test it out and let us know the feedback. Pull Requests with fixes or enhancements are welcomed!","title":"Python"},{"location":"gettingstarted/startworkflow/","text":"Start Workflow Request When starting a Workflow execution with a registered definition, Workflow accepts following parameters: field description Notes name Name of the Workflow. MUST be registered with Conductor before starting workflow version Workflow version defaults to latest available version input JSON object with key value params, that can be used by downstream tasks See Wiring Inputs and Outputs for details correlationId Unique Id that correlates multiple Workflow executions optional taskToDomain See Task Domains for more information. optional workflowDef Provide adhoc Workflow definition to run, without registering. See Dynamic Workflows below. optional externalInputPayloadStoragePath This is taken care of by Java client. See External Payload Storage for more info. optional priority Priority level for the tasks within this workflow execution. Possible values are between 0 - 99. optional Example: Send a POST request to /workflow with payload like: { name : encode_and_deploy , version : 1, correlationId : my_unique_correlation_id , input : { param1 : value1 , param2 : value2 } } Dynamic Workflows If the need arises to run a one-time workflow, and it doesn't make sense to register Task and Workflow definitions in Conductor Server, as it could change dynamically for each execution, dynamic workflow executions can be used. This enables you to provide a workflow definition embedded with the required task definitions to the Start Workflow Request in the workflowDef parameter, avoiding the need to register the blueprints before execution. Example: Send a POST request to /workflow with payload like: { name : my_adhoc_unregistered_workflow , workflowDef : { ownerApp : my_owner_app , ownerEmail : my_owner_email@test.com , createdBy : my_username , name : my_adhoc_unregistered_workflow , description : Test Workflow setup , version : 1, tasks : [ { name : fetch_data , type : HTTP , taskReferenceName : fetch_data , inputParameters : { http_request : { connectionTimeOut : 3600 , readTimeOut : 3600 , uri : ${workflow.input.uri} , method : GET , accept : application/json , content-Type : application/json , headers : { } } }, taskDefinition : { name : fetch_data , retryCount : 0, timeoutSeconds : 3600, timeoutPolicy : TIME_OUT_WF , retryLogic : FIXED , retryDelaySeconds : 0, responseTimeoutSeconds : 3000 } } ], outputParameters : { } }, input : { uri : http://www.google.com } } Note If the taskDefinition is defined with Metadata API, it doesn't have to be added in above dynamic workflow definition.","title":"Start a Workflow"},{"location":"gettingstarted/startworkflow/#start-workflow-request","text":"When starting a Workflow execution with a registered definition, Workflow accepts following parameters: field description Notes name Name of the Workflow. MUST be registered with Conductor before starting workflow version Workflow version defaults to latest available version input JSON object with key value params, that can be used by downstream tasks See Wiring Inputs and Outputs for details correlationId Unique Id that correlates multiple Workflow executions optional taskToDomain See Task Domains for more information. optional workflowDef Provide adhoc Workflow definition to run, without registering. See Dynamic Workflows below. optional externalInputPayloadStoragePath This is taken care of by Java client. See External Payload Storage for more info. optional priority Priority level for the tasks within this workflow execution. Possible values are between 0 - 99. optional Example: Send a POST request to /workflow with payload like: { name : encode_and_deploy , version : 1, correlationId : my_unique_correlation_id , input : { param1 : value1 , param2 : value2 } }","title":"Start Workflow Request"},{"location":"gettingstarted/startworkflow/#dynamic-workflows","text":"If the need arises to run a one-time workflow, and it doesn't make sense to register Task and Workflow definitions in Conductor Server, as it could change dynamically for each execution, dynamic workflow executions can be used. This enables you to provide a workflow definition embedded with the required task definitions to the Start Workflow Request in the workflowDef parameter, avoiding the need to register the blueprints before execution. Example: Send a POST request to /workflow with payload like: { name : my_adhoc_unregistered_workflow , workflowDef : { ownerApp : my_owner_app , ownerEmail : my_owner_email@test.com , createdBy : my_username , name : my_adhoc_unregistered_workflow , description : Test Workflow setup , version : 1, tasks : [ { name : fetch_data , type : HTTP , taskReferenceName : fetch_data , inputParameters : { http_request : { connectionTimeOut : 3600 , readTimeOut : 3600 , uri : ${workflow.input.uri} , method : GET , accept : application/json , content-Type : application/json , headers : { } } }, taskDefinition : { name : fetch_data , retryCount : 0, timeoutSeconds : 3600, timeoutPolicy : TIME_OUT_WF , retryLogic : FIXED , retryDelaySeconds : 0, responseTimeoutSeconds : 3000 } } ], outputParameters : { } }, input : { uri : http://www.google.com } } Note If the taskDefinition is defined with Metadata API, it doesn't have to be added in above dynamic workflow definition.","title":"Dynamic Workflows"},{"location":"labs/beginner/","text":"Hands on mode Please feel free to follow along using any of these resources: Using cURL. Postman or similar REST client. Creating a Workflow Let's create a simple workflow that adds Netflix Idents to videos. We'll be mocking the adding Idents part and focusing on actually executing this process flow. What are Netflix Idents? Netflix Idents are those 4 second videos with Netflix logo, which appears at the beginning and end of shows. Learn more about them here . You might have also noticed they're different for Animation and several other genres. Disclaimer Obviously, this is not how Netflix adds Idents. Those Workflows are indeed very complex. But, it should give you an idea about how Conductor can be used to implement similar features. The workflow in this lab will look like this: This workflow contains the following: Worker Task verify_if_idents_are_added to verify if Idents are already added. Decision Task that takes output from the previous task, and decides whether to schedule the add_idents task. add_idents task which is another worker Task. Creating Task definitions Let's create the task definition for verify_if_idents_are_added in JSON. This task will be a SIMPLE task which is supposed to be executed by an Idents microservice. We'll be mocking the Idents microservice part. Note that at this point, we don't have to specify whether it is a System task or Worker task. We are only specifying the required configurations for the task, like number of times it should be retried, timeouts etc. We shall start by using name parameter for task name. { name : verify_if_idents_are_added } We'd like this task to be retried 3 times on failure. { name : verify_if_idents_are_added , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10 } And to timeout after 300 seconds. i.e. if the task doesn't finish execution within this time limit after transitioning to IN_PROGRESS state, the Conductor server cancels this task and schedules a new execution of this task in the queue. { name : verify_if_idents_are_added , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF } And a responseTimeout of 180 seconds. { name : verify_if_idents_are_added , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF , responseTimeoutSeconds : 180 } We can define several other fields defined here , but this is a good place to start with. Similarly, create another task definition: add_idents . { name : add_idents , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF , responseTimeoutSeconds : 180 } Send a POST request to /metadata/taskdefs endpoint to register these tasks. You can use Swagger, Postman, CURL or similar tools. Why is the Decision Task not registered? System Tasks that are part of control flow do not need to be registered. However, some system tasks where the retries, rate limiting and other mechanisms are required, like HTTP Task, are to be registered though. Important Task and Workflow Definition names are unique. The names we use below might have already been registered. For this lab, add a prefix with your username, {my_username}_verify_if_idents_are_added for example. This is definitely not recommended for Production usage though. Example curl -X POST \\ http://localhost:8080/api/metadata/taskdefs \\ -H 'Content-Type: application/json' \\ -d '[ { name : verify_if_idents_are_added , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF , responseTimeoutSeconds : 180, ownerEmail : type your email here }, { name : add_idents , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF , responseTimeoutSeconds : 180, ownerEmail : type your email here } ]' Creating Workflow Definition Creating Workflow definition is almost similar. We shall use the Task definitions created above. Note that same Task definitions can be used in multiple workflows, or for multipe times in same Workflow (that's where taskReferenceName is useful). A workflow without any tasks looks like this: { name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 1, schemaVersion : 2, tasks : [] } Add the first task that this workflow has to execute. All the tasks must be added to the tasks array. { name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 1, schemaVersion : 2, tasks : [ { name : verify_if_idents_are_added , taskReferenceName : ident_verification , inputParameters : { contentId : ${workflow.input.contentId} }, type : SIMPLE } ] } Wiring Input/Outputs Notice how we were using ${workflow.input.contentId} to pass inputs to this task. Conductor can wire inputs between workflow and tasks, and between tasks. i.e The task verify_if_idents_are_added is wired to accept inputs from the workflow input using JSONPath expression ${workflow.input.param} . Learn more about wiring inputs and outputs here . Let's define decisionCases now. Checkout the Decision task structure here . A Decision task is specified by type:\"DECISION\" , caseValueParam and decisionCases which lists all the branches of Decision task. This is similar to a switch..case but written in Conductor JSON DSL. Adding the decision task: { name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 2, schemaVersion : 2, tasks : [ { name : verify_if_idents_are_added , taskReferenceName : ident_verification , inputParameters : { contentId : ${workflow.input.contentId} }, type : SIMPLE }, { name : decide_task , taskReferenceName : is_idents_added , inputParameters : { case_value_param : ${ident_verification.output.is_idents_added} }, type : DECISION , caseValueParam : case_value_param , decisionCases : { } } ] } Each decision branch could have multiple tasks, so it has to be defined as an array. { name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 2, schemaVersion : 2, tasks : [ { name : verify_if_idents_are_added , taskReferenceName : ident_verification , inputParameters : { contentId : ${workflow.input.contentId} }, type : SIMPLE }, { name : decide_task , taskReferenceName : is_idents_added , inputParameters : { case_value_param : ${ident_verification.output.is_idents_added} }, type : DECISION , caseValueParam : case_value_param , decisionCases : { false : [ { name : add_idents , taskReferenceName : add_idents_by_type , inputParameters : { identType : ${workflow.input.identType} , contentId : ${workflow.input.contentId} }, type : SIMPLE } ] } } ] } Just like the task definitions, register this workflow definition by sending a POST request to /workflow endpoint. Example curl -X POST \\ http://localhost:8080/api/metadata/workflow \\ -H 'Content-Type: application/json' \\ -d '{ name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 2, schemaVersion : 2, tasks : [ { name : verify_if_idents_are_added , taskReferenceName : ident_verification , inputParameters : { contentId : ${workflow.input.contentId} }, type : SIMPLE }, { name : decide_task , taskReferenceName : is_idents_added , inputParameters : { case_value_param : ${ident_verification.output.is_idents_added} }, type : DECISION , caseValueParam : case_value_param , decisionCases : { false : [ { name : add_idents , taskReferenceName : add_idents_by_type , inputParameters : { identType : ${workflow.input.identType} , contentId : ${workflow.input.contentId} }, type : SIMPLE } ] } } ] }' Starting the Workflow Send a POST request to /workflow with: { name : add_netflix_identation , version : 2, correlationId : my_netflix_identation_workflows , input : { identType : animation , contentId : my_unique_content_id } } Example: curl -X POST \\ http://localhost:8080/api/workflow/add_netflix_identation \\ -H 'Content-Type: application/json' \\ -d '{ identType : animation , contentId : my_unique_content_id }' Successful POST request should return a workflow Id, which you can use to find the execution in the UI. Conductor User Interface Open the UI and navigate to the RUNNING tab, the Workflow should be in the state as below: Feel free to explore the various functionalities that the UI exposes. To elaborate on a few: Workflow Task modals (Opens on clicking any of the tasks in the workflow), which includes task I/O, logs and task JSON. Task Details tab, which shows the sequence of task execution, status, start/end time, and link to worker details which executed the task. Input/Output tab shows workflow input and output. Poll for Worker task Now that verify_if_idents_are_added task is in SCHEDULED state, it is the worker's turn to fetch the task, execute it and update Conductor with final status of the task. Ideally, the workers implementing the Client interface would do this process, executing the tasks on real microservices. But, let's mock this part. Send a GET request to /poll endpoint with your task type. For example: curl -X GET \\ http://localhost:8080/api/tasks/poll/verify_if_idents_are_added Return response, add logs We can respond to Conductor with any of the following states: Task has COMPLETED. Task has FAILED. Call back after seconds [Process the task at a later time]. Considering our Ident Service has verified that the Ident's are not yet added to given Content Id, let's return the task status by sending the below POST request to /tasks endpoint, with payload: { workflowInstanceId : {workflowId} , taskId : {taskId} , reasonForIncompletion : , callbackAfterSeconds : 0, workerId : localhost , status : COMPLETED , outputData : { is_idents_added : false } } Example: curl -X POST \\ http://localhost:8080/api/tasks \\ -H 'Content-Type: application/json' \\ -d '{ workflowInstanceId : cb7c5041-aa85-4940-acb4-3bdcfa9f5c5c , taskId : 741f362b-ee9a-47b6-81b5-9bbbd5c4c992 , reasonForIncompletion : , callbackAfterSeconds : 0, workerId : string , status : COMPLETED , outputData : { is_idents_added : false }, logs : [ { log : Ident verification successful for title: {some_title_name}, with Id: {some_id} , createdTime : 1550178825 } ] }' Check logs in UI You can find the logs we just sent by clicking the verify_if_idents_are_added , upon which a modal should open with Logs tab. Why is System task executed, but Worker task is Scheduled. You will notice that Workflow is in the state as below after sending the POST request: Conductor has executed is_idents_added all through it's lifecycle, without us polling, or returning the status of Task. If it is still unclear, is_idents_added is a System task, and System tasks are executed by Conductor Server. But, add_idents is a SIMPLE task. So, the complete lifecyle of this task (Poll, Update) should be handled by a worker to continue with W\\workflow execution. When Conductor has finished executing all the tasks in given flow, the workflow will reach Terminal state (COMPLETED, FAILED, TIMED_OUT etc.) Next steps You can play around this workflow by failing one of the Tasks, restarting or retrying the Workflow, or by tuning the number of retries, timeoutSeconds etc.","title":"Beginner"},{"location":"labs/beginner/#hands-on-mode","text":"Please feel free to follow along using any of these resources: Using cURL. Postman or similar REST client.","title":"Hands on mode"},{"location":"labs/beginner/#creating-a-workflow","text":"Let's create a simple workflow that adds Netflix Idents to videos. We'll be mocking the adding Idents part and focusing on actually executing this process flow. What are Netflix Idents? Netflix Idents are those 4 second videos with Netflix logo, which appears at the beginning and end of shows. Learn more about them here . You might have also noticed they're different for Animation and several other genres. Disclaimer Obviously, this is not how Netflix adds Idents. Those Workflows are indeed very complex. But, it should give you an idea about how Conductor can be used to implement similar features. The workflow in this lab will look like this: This workflow contains the following: Worker Task verify_if_idents_are_added to verify if Idents are already added. Decision Task that takes output from the previous task, and decides whether to schedule the add_idents task. add_idents task which is another worker Task.","title":"Creating a Workflow"},{"location":"labs/beginner/#creating-task-definitions","text":"Let's create the task definition for verify_if_idents_are_added in JSON. This task will be a SIMPLE task which is supposed to be executed by an Idents microservice. We'll be mocking the Idents microservice part. Note that at this point, we don't have to specify whether it is a System task or Worker task. We are only specifying the required configurations for the task, like number of times it should be retried, timeouts etc. We shall start by using name parameter for task name. { name : verify_if_idents_are_added } We'd like this task to be retried 3 times on failure. { name : verify_if_idents_are_added , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10 } And to timeout after 300 seconds. i.e. if the task doesn't finish execution within this time limit after transitioning to IN_PROGRESS state, the Conductor server cancels this task and schedules a new execution of this task in the queue. { name : verify_if_idents_are_added , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF } And a responseTimeout of 180 seconds. { name : verify_if_idents_are_added , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF , responseTimeoutSeconds : 180 } We can define several other fields defined here , but this is a good place to start with. Similarly, create another task definition: add_idents . { name : add_idents , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF , responseTimeoutSeconds : 180 } Send a POST request to /metadata/taskdefs endpoint to register these tasks. You can use Swagger, Postman, CURL or similar tools. Why is the Decision Task not registered? System Tasks that are part of control flow do not need to be registered. However, some system tasks where the retries, rate limiting and other mechanisms are required, like HTTP Task, are to be registered though. Important Task and Workflow Definition names are unique. The names we use below might have already been registered. For this lab, add a prefix with your username, {my_username}_verify_if_idents_are_added for example. This is definitely not recommended for Production usage though. Example curl -X POST \\ http://localhost:8080/api/metadata/taskdefs \\ -H 'Content-Type: application/json' \\ -d '[ { name : verify_if_idents_are_added , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF , responseTimeoutSeconds : 180, ownerEmail : type your email here }, { name : add_idents , retryCount : 3, retryLogic : FIXED , retryDelaySeconds : 10, timeoutSeconds : 300, timeoutPolicy : TIME_OUT_WF , responseTimeoutSeconds : 180, ownerEmail : type your email here } ]'","title":"Creating Task definitions"},{"location":"labs/beginner/#creating-workflow-definition","text":"Creating Workflow definition is almost similar. We shall use the Task definitions created above. Note that same Task definitions can be used in multiple workflows, or for multipe times in same Workflow (that's where taskReferenceName is useful). A workflow without any tasks looks like this: { name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 1, schemaVersion : 2, tasks : [] } Add the first task that this workflow has to execute. All the tasks must be added to the tasks array. { name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 1, schemaVersion : 2, tasks : [ { name : verify_if_idents_are_added , taskReferenceName : ident_verification , inputParameters : { contentId : ${workflow.input.contentId} }, type : SIMPLE } ] } Wiring Input/Outputs Notice how we were using ${workflow.input.contentId} to pass inputs to this task. Conductor can wire inputs between workflow and tasks, and between tasks. i.e The task verify_if_idents_are_added is wired to accept inputs from the workflow input using JSONPath expression ${workflow.input.param} . Learn more about wiring inputs and outputs here . Let's define decisionCases now. Checkout the Decision task structure here . A Decision task is specified by type:\"DECISION\" , caseValueParam and decisionCases which lists all the branches of Decision task. This is similar to a switch..case but written in Conductor JSON DSL. Adding the decision task: { name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 2, schemaVersion : 2, tasks : [ { name : verify_if_idents_are_added , taskReferenceName : ident_verification , inputParameters : { contentId : ${workflow.input.contentId} }, type : SIMPLE }, { name : decide_task , taskReferenceName : is_idents_added , inputParameters : { case_value_param : ${ident_verification.output.is_idents_added} }, type : DECISION , caseValueParam : case_value_param , decisionCases : { } } ] } Each decision branch could have multiple tasks, so it has to be defined as an array. { name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 2, schemaVersion : 2, tasks : [ { name : verify_if_idents_are_added , taskReferenceName : ident_verification , inputParameters : { contentId : ${workflow.input.contentId} }, type : SIMPLE }, { name : decide_task , taskReferenceName : is_idents_added , inputParameters : { case_value_param : ${ident_verification.output.is_idents_added} }, type : DECISION , caseValueParam : case_value_param , decisionCases : { false : [ { name : add_idents , taskReferenceName : add_idents_by_type , inputParameters : { identType : ${workflow.input.identType} , contentId : ${workflow.input.contentId} }, type : SIMPLE } ] } } ] } Just like the task definitions, register this workflow definition by sending a POST request to /workflow endpoint. Example curl -X POST \\ http://localhost:8080/api/metadata/workflow \\ -H 'Content-Type: application/json' \\ -d '{ name : add_netflix_identation , description : Adds Netflix Identation to video files. , version : 2, schemaVersion : 2, tasks : [ { name : verify_if_idents_are_added , taskReferenceName : ident_verification , inputParameters : { contentId : ${workflow.input.contentId} }, type : SIMPLE }, { name : decide_task , taskReferenceName : is_idents_added , inputParameters : { case_value_param : ${ident_verification.output.is_idents_added} }, type : DECISION , caseValueParam : case_value_param , decisionCases : { false : [ { name : add_idents , taskReferenceName : add_idents_by_type , inputParameters : { identType : ${workflow.input.identType} , contentId : ${workflow.input.contentId} }, type : SIMPLE } ] } } ] }'","title":"Creating Workflow Definition"},{"location":"labs/beginner/#starting-the-workflow","text":"Send a POST request to /workflow with: { name : add_netflix_identation , version : 2, correlationId : my_netflix_identation_workflows , input : { identType : animation , contentId : my_unique_content_id } } Example: curl -X POST \\ http://localhost:8080/api/workflow/add_netflix_identation \\ -H 'Content-Type: application/json' \\ -d '{ identType : animation , contentId : my_unique_content_id }' Successful POST request should return a workflow Id, which you can use to find the execution in the UI.","title":"Starting the Workflow"},{"location":"labs/beginner/#conductor-user-interface","text":"Open the UI and navigate to the RUNNING tab, the Workflow should be in the state as below: Feel free to explore the various functionalities that the UI exposes. To elaborate on a few: Workflow Task modals (Opens on clicking any of the tasks in the workflow), which includes task I/O, logs and task JSON. Task Details tab, which shows the sequence of task execution, status, start/end time, and link to worker details which executed the task. Input/Output tab shows workflow input and output.","title":"Conductor User Interface"},{"location":"labs/beginner/#poll-for-worker-task","text":"Now that verify_if_idents_are_added task is in SCHEDULED state, it is the worker's turn to fetch the task, execute it and update Conductor with final status of the task. Ideally, the workers implementing the Client interface would do this process, executing the tasks on real microservices. But, let's mock this part. Send a GET request to /poll endpoint with your task type. For example: curl -X GET \\ http://localhost:8080/api/tasks/poll/verify_if_idents_are_added","title":"Poll for Worker task"},{"location":"labs/beginner/#return-response-add-logs","text":"We can respond to Conductor with any of the following states: Task has COMPLETED. Task has FAILED. Call back after seconds [Process the task at a later time]. Considering our Ident Service has verified that the Ident's are not yet added to given Content Id, let's return the task status by sending the below POST request to /tasks endpoint, with payload: { workflowInstanceId : {workflowId} , taskId : {taskId} , reasonForIncompletion : , callbackAfterSeconds : 0, workerId : localhost , status : COMPLETED , outputData : { is_idents_added : false } } Example: curl -X POST \\ http://localhost:8080/api/tasks \\ -H 'Content-Type: application/json' \\ -d '{ workflowInstanceId : cb7c5041-aa85-4940-acb4-3bdcfa9f5c5c , taskId : 741f362b-ee9a-47b6-81b5-9bbbd5c4c992 , reasonForIncompletion : , callbackAfterSeconds : 0, workerId : string , status : COMPLETED , outputData : { is_idents_added : false }, logs : [ { log : Ident verification successful for title: {some_title_name}, with Id: {some_id} , createdTime : 1550178825 } ] }' Check logs in UI You can find the logs we just sent by clicking the verify_if_idents_are_added , upon which a modal should open with Logs tab.","title":"Return response, add logs"},{"location":"labs/beginner/#why-is-system-task-executed-but-worker-task-is-scheduled","text":"You will notice that Workflow is in the state as below after sending the POST request: Conductor has executed is_idents_added all through it's lifecycle, without us polling, or returning the status of Task. If it is still unclear, is_idents_added is a System task, and System tasks are executed by Conductor Server. But, add_idents is a SIMPLE task. So, the complete lifecyle of this task (Poll, Update) should be handled by a worker to continue with W\\workflow execution. When Conductor has finished executing all the tasks in given flow, the workflow will reach Terminal state (COMPLETED, FAILED, TIMED_OUT etc.)","title":"Why is System task executed, but Worker task is Scheduled."},{"location":"labs/beginner/#next-steps","text":"You can play around this workflow by failing one of the Tasks, restarting or retrying the Workflow, or by tuning the number of retries, timeoutSeconds etc.","title":"Next steps"},{"location":"labs/eventhandlers/","text":"About In this Lab, we shall: Publish an Event to Conductor using Event task. Subscribe to Events, and perform actions: Start a Workflow Complete Task Conductor Supports Eventing with two Interfaces: Event Task Event Handlers We shall create a simple cyclic workflow similar to this: Create Workflow Definitions Let's create two workflows: test_workflow_for_eventHandler which will have an Event task to start another workflow, and a WAIT System task that will be completed by an event. test_workflow_startedBy_eventHandler which will have an Event task to generate an event to complete WAIT task in the above workflow. Send POST requests to /metadata/workflow endpoint with below payloads: { name : test_workflow_for_eventHandler , description : A test workflow to start another workflow with EventHandler , version : 1, tasks : [ { name : test_start_workflow_event , taskReferenceName : start_workflow_with_event , type : EVENT , sink : conductor }, { name : test_task_tobe_completed_by_eventHandler , taskReferenceName : test_task_tobe_completed_by_eventHandler , type : WAIT } ] } { name : test_workflow_startedBy_eventHandler , description : A test workflow which is started by EventHandler, and then goes on to complete task in another workflow. , version : 1, tasks : [ { name : test_complete_task_event , taskReferenceName : complete_task_with_event , inputParameters : { sourceWorkflowId : ${workflow.input.sourceWorkflowId} }, type : EVENT , sink : conductor } ] } Event Tasks in Workflow EVENT task is a System task, and we shall define it just like other Tasks in Workflow, with sink parameter. Also, EVENT task doesn't have to be registered before using in Workflow. This is also true for the WAIT task. Hence, we will not be registering any tasks for these workflows. Events are sent, but they're not handled (yet) Once you try to start test_workflow_for_eventHandler workflow, you would notice that the event is sent successfully, but the second worflow test_workflow_startedBy_eventHandler is not started. We have sent the Events, but we also need to define Event Handlers for Conductor to take any actions based on the Event. Let's create Event Handlers . Create Event Handlers Event Handler definitions are pretty much like Task or Workflow definitions. We start by name: { name : test_start_workflow } Event Handler should know the Queue it has to listen to. This should be defined in event parameter. When using Conductor queues, define event with format: conductor:{workflow_name}:{taskReferenceName} And when using SQS, define with format: sqs:{my_sqs_queue_name} { name : test_start_workflow , event : conductor:test_workflow_for_eventHandler:start_workflow_with_event } Event Handler can perform a list of actions defined in actions array parameter, for this particular event queue. { name : test_start_workflow , event : conductor:test_workflow_for_eventHandler:start_workflow_with_event , actions : [ insert-actions-here ], active : true } Let's define start_workflow action. We shall pass the name of workflow we would like to start. The start_workflow parameter can use any of the values from the general Start Workflow Request . Here we are passing in the workflowId, so that the Complete Task Event Handler can use it. { action : start_workflow , start_workflow : { name : test_workflow_startedBy_eventHandler , input : { sourceWorkflowId : ${workflowInstanceId} } } } Send a POST request to /event endpoint: { name : test_start_workflow , event : conductor:test_workflow_for_eventHandler:start_workflow_with_event , actions : [ { action : start_workflow , start_workflow : { name : test_workflow_startedBy_eventHandler , input : { sourceWorkflowId : ${workflowInstanceId} } } } ], active : true } Similarly, create another Event Handler to complete task. { name : test_complete_task_event , event : conductor:test_workflow_startedBy_eventHandler:complete_task_with_event , actions : [ { action : complete_task , complete_task : { workflowId : ${sourceWorkflowId} , taskRefName : test_task_tobe_completed_by_eventHandler } } ], active : true } Final flow of Workflow After wiring all of the above, starting the test_workflow_for_eventHandler should: Start test_workflow_startedBy_eventHandler workflow. Sets test_task_tobe_completed_by_eventHandler WAIT task IN_PROGRESS . test_workflow_startedBy_eventHandler event task would publish an Event to complete the WAIT task above. Both the workflows would move to COMPLETED state.","title":"Events and Event Handlers"},{"location":"labs/eventhandlers/#about","text":"In this Lab, we shall: Publish an Event to Conductor using Event task. Subscribe to Events, and perform actions: Start a Workflow Complete Task Conductor Supports Eventing with two Interfaces: Event Task Event Handlers We shall create a simple cyclic workflow similar to this:","title":"About"},{"location":"labs/eventhandlers/#create-workflow-definitions","text":"Let's create two workflows: test_workflow_for_eventHandler which will have an Event task to start another workflow, and a WAIT System task that will be completed by an event. test_workflow_startedBy_eventHandler which will have an Event task to generate an event to complete WAIT task in the above workflow. Send POST requests to /metadata/workflow endpoint with below payloads: { name : test_workflow_for_eventHandler , description : A test workflow to start another workflow with EventHandler , version : 1, tasks : [ { name : test_start_workflow_event , taskReferenceName : start_workflow_with_event , type : EVENT , sink : conductor }, { name : test_task_tobe_completed_by_eventHandler , taskReferenceName : test_task_tobe_completed_by_eventHandler , type : WAIT } ] } { name : test_workflow_startedBy_eventHandler , description : A test workflow which is started by EventHandler, and then goes on to complete task in another workflow. , version : 1, tasks : [ { name : test_complete_task_event , taskReferenceName : complete_task_with_event , inputParameters : { sourceWorkflowId : ${workflow.input.sourceWorkflowId} }, type : EVENT , sink : conductor } ] }","title":"Create Workflow Definitions"},{"location":"labs/eventhandlers/#event-tasks-in-workflow","text":"EVENT task is a System task, and we shall define it just like other Tasks in Workflow, with sink parameter. Also, EVENT task doesn't have to be registered before using in Workflow. This is also true for the WAIT task. Hence, we will not be registering any tasks for these workflows.","title":"Event Tasks in Workflow"},{"location":"labs/eventhandlers/#events-are-sent-but-theyre-not-handled-yet","text":"Once you try to start test_workflow_for_eventHandler workflow, you would notice that the event is sent successfully, but the second worflow test_workflow_startedBy_eventHandler is not started. We have sent the Events, but we also need to define Event Handlers for Conductor to take any actions based on the Event. Let's create Event Handlers .","title":"Events are sent, but they're not handled (yet)"},{"location":"labs/eventhandlers/#create-event-handlers","text":"Event Handler definitions are pretty much like Task or Workflow definitions. We start by name: { name : test_start_workflow } Event Handler should know the Queue it has to listen to. This should be defined in event parameter. When using Conductor queues, define event with format: conductor:{workflow_name}:{taskReferenceName} And when using SQS, define with format: sqs:{my_sqs_queue_name} { name : test_start_workflow , event : conductor:test_workflow_for_eventHandler:start_workflow_with_event } Event Handler can perform a list of actions defined in actions array parameter, for this particular event queue. { name : test_start_workflow , event : conductor:test_workflow_for_eventHandler:start_workflow_with_event , actions : [ insert-actions-here ], active : true } Let's define start_workflow action. We shall pass the name of workflow we would like to start. The start_workflow parameter can use any of the values from the general Start Workflow Request . Here we are passing in the workflowId, so that the Complete Task Event Handler can use it. { action : start_workflow , start_workflow : { name : test_workflow_startedBy_eventHandler , input : { sourceWorkflowId : ${workflowInstanceId} } } } Send a POST request to /event endpoint: { name : test_start_workflow , event : conductor:test_workflow_for_eventHandler:start_workflow_with_event , actions : [ { action : start_workflow , start_workflow : { name : test_workflow_startedBy_eventHandler , input : { sourceWorkflowId : ${workflowInstanceId} } } } ], active : true } Similarly, create another Event Handler to complete task. { name : test_complete_task_event , event : conductor:test_workflow_startedBy_eventHandler:complete_task_with_event , actions : [ { action : complete_task , complete_task : { workflowId : ${sourceWorkflowId} , taskRefName : test_task_tobe_completed_by_eventHandler } } ], active : true }","title":"Create Event Handlers"},{"location":"labs/eventhandlers/#final-flow-of-workflow","text":"After wiring all of the above, starting the test_workflow_for_eventHandler should: Start test_workflow_startedBy_eventHandler workflow. Sets test_task_tobe_completed_by_eventHandler WAIT task IN_PROGRESS . test_workflow_startedBy_eventHandler event task would publish an Event to complete the WAIT task above. Both the workflows would move to COMPLETED state.","title":"Final flow of Workflow"},{"location":"labs/kitchensink/","text":"An example kitchensink workflow that demonstrates the usage of all the schema constructs. Definition { name : kitchensink , description : kitchensink workflow , version : 1, tasks : [ { name : task_1 , taskReferenceName : task_1 , inputParameters : { mod : ${workflow.input.mod} , oddEven : ${workflow.input.oddEven} }, type : SIMPLE }, { name : event_task , taskReferenceName : event_0 , inputParameters : { mod : ${workflow.input.mod} , oddEven : ${workflow.input.oddEven} }, type : EVENT , sink : conductor }, { name : dyntask , taskReferenceName : task_2 , inputParameters : { taskToExecute : ${workflow.input.task2Name} }, type : DYNAMIC , dynamicTaskNameParam : taskToExecute }, { name : oddEvenDecision , taskReferenceName : oddEvenDecision , inputParameters : { oddEven : ${task_2.output.oddEven} }, type : DECISION , caseValueParam : oddEven , decisionCases : { 0 : [ { name : task_4 , taskReferenceName : task_4 , inputParameters : { mod : ${task_2.output.mod} , oddEven : ${task_2.output.oddEven} }, type : SIMPLE }, { name : dynamic_fanout , taskReferenceName : fanout1 , inputParameters : { dynamicTasks : ${task_4.output.dynamicTasks} , input : ${task_4.output.inputs} }, type : FORK_JOIN_DYNAMIC , dynamicForkTasksParam : dynamicTasks , dynamicForkTasksInputParamName : input }, { name : dynamic_join , taskReferenceName : join1 , type : JOIN } ], 1 : [ { name : fork_join , taskReferenceName : forkx , type : FORK_JOIN , forkTasks : [ [ { name : task_10 , taskReferenceName : task_10 , type : SIMPLE }, { name : sub_workflow_x , taskReferenceName : wf3 , inputParameters : { mod : ${task_1.output.mod} , oddEven : ${task_1.output.oddEven} }, type : SUB_WORKFLOW , subWorkflowParam : { name : sub_flow_1 , version : 1 } } ], [ { name : task_11 , taskReferenceName : task_11 , type : SIMPLE }, { name : sub_workflow_x , taskReferenceName : wf4 , inputParameters : { mod : ${task_1.output.mod} , oddEven : ${task_1.output.oddEven} }, type : SUB_WORKFLOW , subWorkflowParam : { name : sub_flow_1 , version : 1 } } ] ] }, { name : join , taskReferenceName : join2 , type : JOIN , joinOn : [ wf3 , wf4 ] } ] } }, { name : search_elasticsearch , taskReferenceName : get_es_1 , inputParameters : { http_request : { uri : http://localhost:9200/conductor/_search?size=10 , method : GET } }, type : HTTP }, { name : task_30 , taskReferenceName : task_30 , inputParameters : { statuses : ${get_es_1.output..status} , workflowIds : ${get_es_1.output..workflowId} }, type : SIMPLE } ], outputParameters : { statues : ${get_es_1.output..status} , workflowIds : ${get_es_1.output..workflowId} }, ownerEmail : example@email.com , schemaVersion : 2 } Visual Flow Running Kitchensink Workflow Start the server as documented here . Use -DloadSample=true java system property when launching the server. This will create a kitchensink workflow, related task definitions and kick off an instance of kitchensink workflow. Once the workflow has started, the first task remains in the SCHEDULED state. This is because no workers are currently polling for the task. We will use the REST endpoints directly to poll for tasks and updating the status. Start workflow execution Start the execution of the kitchensink workflow by posting the following: curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d ' { task2Name : task_5 } ' The response is a text string identifying the workflow instance id. Poll for the first task: curl http://localhost:8080/api/tasks/poll/task_1 The response should look something like: { taskType : task_1 , status : IN_PROGRESS , inputData : { mod : null, oddEven : null }, referenceTaskName : task_1 , retryCount : 0, seq : 1, pollCount : 1, taskDefName : task_1 , scheduledTime : 1486580932471, startTime : 1486580933869, endTime : 0, updateTime : 1486580933902, startDelayInSeconds : 0, retried : false, callbackFromWorker : true, responseTimeoutSeconds : 3600, workflowInstanceId : b0d1a935-3d74-46fd-92b2-0ca1e388659f , taskId : b9eea7dd-3fbd-46b9-a9ff-b00279459476 , callbackAfterSeconds : 0, polledTime : 1486580933902, queueWaitTime : 1398 } Update the task status Note the values for taskId and workflowInstanceId fields from the poll response Update the status of the task as COMPLETED as below: curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d ' { taskId : b9eea7dd-3fbd-46b9-a9ff-b00279459476 , workflowInstanceId : b0d1a935-3d74-46fd-92b2-0ca1e388659f , status : COMPLETED , outputData : { mod : 5, taskToExecute : task_1 , oddEven : 0, dynamicTasks : [ { name : task_1 , taskReferenceName : task_1_1 , type : SIMPLE }, { name : sub_workflow_4 , taskReferenceName : wf_dyn , type : SUB_WORKFLOW , subWorkflowParam : { name : sub_flow_1 } } ], inputs : { task_1_1 : {}, wf_dyn : {} } } }' This will mark the task_1 as completed and schedule task_5 as the next task. Repeat the same process for the subsequently scheduled tasks until the completion.","title":"Kitchensink"},{"location":"labs/kitchensink/#definition","text":"{ name : kitchensink , description : kitchensink workflow , version : 1, tasks : [ { name : task_1 , taskReferenceName : task_1 , inputParameters : { mod : ${workflow.input.mod} , oddEven : ${workflow.input.oddEven} }, type : SIMPLE }, { name : event_task , taskReferenceName : event_0 , inputParameters : { mod : ${workflow.input.mod} , oddEven : ${workflow.input.oddEven} }, type : EVENT , sink : conductor }, { name : dyntask , taskReferenceName : task_2 , inputParameters : { taskToExecute : ${workflow.input.task2Name} }, type : DYNAMIC , dynamicTaskNameParam : taskToExecute }, { name : oddEvenDecision , taskReferenceName : oddEvenDecision , inputParameters : { oddEven : ${task_2.output.oddEven} }, type : DECISION , caseValueParam : oddEven , decisionCases : { 0 : [ { name : task_4 , taskReferenceName : task_4 , inputParameters : { mod : ${task_2.output.mod} , oddEven : ${task_2.output.oddEven} }, type : SIMPLE }, { name : dynamic_fanout , taskReferenceName : fanout1 , inputParameters : { dynamicTasks : ${task_4.output.dynamicTasks} , input : ${task_4.output.inputs} }, type : FORK_JOIN_DYNAMIC , dynamicForkTasksParam : dynamicTasks , dynamicForkTasksInputParamName : input }, { name : dynamic_join , taskReferenceName : join1 , type : JOIN } ], 1 : [ { name : fork_join , taskReferenceName : forkx , type : FORK_JOIN , forkTasks : [ [ { name : task_10 , taskReferenceName : task_10 , type : SIMPLE }, { name : sub_workflow_x , taskReferenceName : wf3 , inputParameters : { mod : ${task_1.output.mod} , oddEven : ${task_1.output.oddEven} }, type : SUB_WORKFLOW , subWorkflowParam : { name : sub_flow_1 , version : 1 } } ], [ { name : task_11 , taskReferenceName : task_11 , type : SIMPLE }, { name : sub_workflow_x , taskReferenceName : wf4 , inputParameters : { mod : ${task_1.output.mod} , oddEven : ${task_1.output.oddEven} }, type : SUB_WORKFLOW , subWorkflowParam : { name : sub_flow_1 , version : 1 } } ] ] }, { name : join , taskReferenceName : join2 , type : JOIN , joinOn : [ wf3 , wf4 ] } ] } }, { name : search_elasticsearch , taskReferenceName : get_es_1 , inputParameters : { http_request : { uri : http://localhost:9200/conductor/_search?size=10 , method : GET } }, type : HTTP }, { name : task_30 , taskReferenceName : task_30 , inputParameters : { statuses : ${get_es_1.output..status} , workflowIds : ${get_es_1.output..workflowId} }, type : SIMPLE } ], outputParameters : { statues : ${get_es_1.output..status} , workflowIds : ${get_es_1.output..workflowId} }, ownerEmail : example@email.com , schemaVersion : 2 }","title":"Definition"},{"location":"labs/kitchensink/#visual-flow","text":"","title":"Visual Flow"},{"location":"labs/kitchensink/#running-kitchensink-workflow","text":"Start the server as documented here . Use -DloadSample=true java system property when launching the server. This will create a kitchensink workflow, related task definitions and kick off an instance of kitchensink workflow. Once the workflow has started, the first task remains in the SCHEDULED state. This is because no workers are currently polling for the task. We will use the REST endpoints directly to poll for tasks and updating the status.","title":"Running Kitchensink Workflow"},{"location":"labs/kitchensink/#start-workflow-execution","text":"Start the execution of the kitchensink workflow by posting the following: curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d ' { task2Name : task_5 } ' The response is a text string identifying the workflow instance id.","title":"Start workflow execution"},{"location":"labs/kitchensink/#poll-for-the-first-task","text":"curl http://localhost:8080/api/tasks/poll/task_1 The response should look something like: { taskType : task_1 , status : IN_PROGRESS , inputData : { mod : null, oddEven : null }, referenceTaskName : task_1 , retryCount : 0, seq : 1, pollCount : 1, taskDefName : task_1 , scheduledTime : 1486580932471, startTime : 1486580933869, endTime : 0, updateTime : 1486580933902, startDelayInSeconds : 0, retried : false, callbackFromWorker : true, responseTimeoutSeconds : 3600, workflowInstanceId : b0d1a935-3d74-46fd-92b2-0ca1e388659f , taskId : b9eea7dd-3fbd-46b9-a9ff-b00279459476 , callbackAfterSeconds : 0, polledTime : 1486580933902, queueWaitTime : 1398 }","title":"Poll for the first task:"},{"location":"labs/kitchensink/#update-the-task-status","text":"Note the values for taskId and workflowInstanceId fields from the poll response Update the status of the task as COMPLETED as below: curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d ' { taskId : b9eea7dd-3fbd-46b9-a9ff-b00279459476 , workflowInstanceId : b0d1a935-3d74-46fd-92b2-0ca1e388659f , status : COMPLETED , outputData : { mod : 5, taskToExecute : task_1 , oddEven : 0, dynamicTasks : [ { name : task_1 , taskReferenceName : task_1_1 , type : SIMPLE }, { name : sub_workflow_4 , taskReferenceName : wf_dyn , type : SUB_WORKFLOW , subWorkflowParam : { name : sub_flow_1 } } ], inputs : { task_1_1 : {}, wf_dyn : {} } } }' This will mark the task_1 as completed and schedule task_5 as the next task. Repeat the same process for the subsequently scheduled tasks until the completion.","title":"Update the task status"},{"location":"metrics/client/","text":"When using the Java client, the following metrics are published: Name Purpose Tags task_execution_queue_full Counter to record execution queue has saturated taskType task_poll_error Client error when polling for a task queue taskType, includeRetries, status task_paused Counter for number of times the task has been polled, when the worker has been paused taskType task_execute_error Execution error taskType task_ack_failed Task ack failed taskType task_ack_error Task ack has encountered an exception taskType task_update_error Task status cannot be updated back to server taskType task_poll_counter Incremented each time polling is done taskType task_poll_time Time to poll for a batch of tasks taskType task_execute_time Time to execute a task taskType task_result_size Records output payload size of a task taskType workflow_input_size Records input payload size of a workflow workflowType, workflowVersion external_payload_used Incremented each time external payload storage is used name, operation, payloadType Metrics on client side supplements the one collected from server in identifying the network as well as client side issues.","title":"Client Metrics"},{"location":"metrics/server/","text":"Publishing metrics Conductor uses spectator to collect the metrics. To enable conductor serve to publish metrics, add this dependency to your build.gradle. Conductor Server enables you to load additional modules dynamically, this feature can be controlled using this configuration . Create your own AbstractModule that overides configure function and registers the Spectator metrics registry. Initialize the Registry and add it to the global registry via ((CompositeRegistry)Spectator.globalRegistry()).add(...) . The following metrics are published by the server. You can use these metrics to configure alerts for your workflows and tasks. Name Purpose Tags workflow_server_error Rate at which server side error is happening methodName workflow_failure Counter for failing workflows workflowName, status workflow_start_error Counter for failing to start a workflow workflowName workflow_running Counter for no. of running workflows workflowName, version workflow_execution Timer for Workflow completion workflowName, ownerApp task_queue_wait Time spent by a task in queue taskType task_execution Time taken to execute a task taskType, includeRetries, status task_poll Time taken to poll for a task taskType task_poll_count Counter for number of times the task is being polled taskType, domain task_queue_depth Pending tasks queue depth taskType, ownerApp task_rate_limited Current number of tasks being rate limited taskType task_concurrent_execution_limited Current number of tasks being limited by concurrent execution limit taskType task_timeout Counter for timed out tasks taskType task_response_timeout Counter for tasks timedout due to responseTimeout taskType task_update_conflict Counter for task update conflicts. Eg: when the workflow is in terminal state workflowName, taskType, taskStatus, workflowStatus event_queue_messages_processed Counter for number of messages fetched from an event queue queueType, queueName observable_queue_error Counter for number of errors encountered when fetching messages from an event queue queueType event_queue_messages_handled Counter for number of messages executed from an event queue queueType, queueName external_payload_storage_usage Counter for number of times external payload storage was used name, operation, payloadType Collecting metrics One way of collecting metrics is to push them into the logging framework (log4j). Log4j supports various appenders that can print metrics into a console/file or even send them to remote metrics collectors over e.g. syslog channel. Conductor provides optional modules that connect metrics registry with the logging framework. To enable these modules, configure following additional modules property in config.properties: conductor.additional.modules=com.netflix.conductor.contribs.metrics.MetricsRegistryModule,com.netflix.conductor.contribs.metrics.LoggingMetricsModule com.netflix.conductor.contribs.metrics.LoggingMetricsModule.reportPeriodSeconds=15 This will push all available metrics into log4j every 15 seconds. By default, the metrics will be handled as a regular log message (just printed to console with default log4j.properties). In order to change that, you can use following log4j configuration that prints metrics into a dedicated file: log4j.rootLogger=INFO,console,file log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.File=/app/logs/conductor.log log4j.appender.file.MaxFileSize=10MB log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n # Dedicated file appender for metrics log4j.appender.fileMetrics=org.apache.log4j.RollingFileAppender log4j.appender.fileMetrics.File=/app/logs/metrics.log log4j.appender.fileMetrics.MaxFileSize=10MB log4j.appender.fileMetrics.MaxBackupIndex=10 log4j.appender.fileMetrics.layout=org.apache.log4j.PatternLayout log4j.appender.fileMetrics.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.logger.ConductorMetrics=INFO,console,fileMetrics log4j.additivity.ConductorMetrics=false This configuration is bundled with conductor-server in file: log4j-file-appender.properties and can be utilized by setting env var: LOG4J_PROP=log4j-file-appender.properties This variable is used by startup.sh script. Integration with logstash using a log file The metrics collected by log4j can be further processed and pushed into a central collector such as ElasticSearch. One way of achieving this is to use: log4j file appender - logstash - ElasticSearch. Considering the above setup, you can deploy logstash to consume the contents of /app/logs/metrics.log file, process it and send further to elasticsearch. Following configuration needs to be used in logstash to achieve it: pipeline.yml: - pipeline.id: conductor_metrics path.config: /usr/share/logstash/pipeline/logstash_metrics.conf pipeline.workers: 2 logstash_metrics.conf input { file { path = [ /conductor-server-logs/metrics.log ] codec = multiline { pattern = ^%{TIMESTAMP_ISO8601} negate = true what = previous } } } filter { kv { field_split = , include_keys = [ name , type , count , value ] } mutate { convert = { count = integer value = float } } } output { elasticsearch { hosts = [ elasticsearch:9200 ] } } Note: In addition to forwarding the metrics into ElasticSearch, logstash will extract following fields from each metric: name, type, count, value and set proper types Integration with fluentd using a syslog channel Another example of metrics collection uses: log4j syslog appender - fluentd - prometheus. In this case, a specific log4j properties file needs to be used so that metrics are pushed into a syslog channel: log4j.rootLogger=INFO,console,file log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.File=/app/logs/conductor.log log4j.appender.file.MaxFileSize=10MB log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n # Syslog based appender streaming metrics into fluentd log4j.appender.server=org.apache.log4j.net.SyslogAppender log4j.appender.server.syslogHost=fluentd:5170 log4j.appender.server.facility=LOCAL1 log4j.appender.server.layout=org.apache.log4j.PatternLayout log4j.appender.server.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.logger.ConductorMetrics=INFO,console,server log4j.additivity.ConductorMetrics=false And on the fluentd side you need following configuration: source @type prometheus /source source @type syslog port 5170 bind 0.0.0.0 tag conductor parse ; only allow TIMER metrics of workflow execution and extract tenant ID @type regexp expression /^.*type=TIMER, name=workflow_execution.class-WorkflowMonitor.+workflowName-(? tenant .*)_(? workflow .+), count=(? count \\d+), min=(? min [\\d.]+), max=(? max [\\d.]+), mean=(? mean [\\d.]+).*$/ types count:integer,min:float,max:float,mean:float /parse /source filter conductor.local1.info @type prometheus metric name conductor_workflow_count type gauge desc The total number of executed workflows key count labels workflow ${workflow} tenant ${tenant} user ${email} /labels /metric metric name conductor_workflow_max_duration type gauge desc Max duration in millis for a workflow key max labels workflow ${workflow} tenant ${tenant} user ${email} /labels /metric metric name conductor_workflow_mean_duration type gauge desc Mean duration in millis for a workflow key mean labels workflow ${workflow} tenant ${tenant} user ${email} /labels /metric /filter match ** @type stdout /match With above configuration, fluentd will: - Listen to raw metrics on 0.0.0.0:5170 - Collect only workflow_execution TIMER metrics - Process the raw metrics and expose 3 prometheus specific metrics - Expose prometheus metrics on http://fluentd:24231/metrics","title":"Server Metrics"},{"location":"metrics/server/#publishing-metrics","text":"Conductor uses spectator to collect the metrics. To enable conductor serve to publish metrics, add this dependency to your build.gradle. Conductor Server enables you to load additional modules dynamically, this feature can be controlled using this configuration . Create your own AbstractModule that overides configure function and registers the Spectator metrics registry. Initialize the Registry and add it to the global registry via ((CompositeRegistry)Spectator.globalRegistry()).add(...) . The following metrics are published by the server. You can use these metrics to configure alerts for your workflows and tasks. Name Purpose Tags workflow_server_error Rate at which server side error is happening methodName workflow_failure Counter for failing workflows workflowName, status workflow_start_error Counter for failing to start a workflow workflowName workflow_running Counter for no. of running workflows workflowName, version workflow_execution Timer for Workflow completion workflowName, ownerApp task_queue_wait Time spent by a task in queue taskType task_execution Time taken to execute a task taskType, includeRetries, status task_poll Time taken to poll for a task taskType task_poll_count Counter for number of times the task is being polled taskType, domain task_queue_depth Pending tasks queue depth taskType, ownerApp task_rate_limited Current number of tasks being rate limited taskType task_concurrent_execution_limited Current number of tasks being limited by concurrent execution limit taskType task_timeout Counter for timed out tasks taskType task_response_timeout Counter for tasks timedout due to responseTimeout taskType task_update_conflict Counter for task update conflicts. Eg: when the workflow is in terminal state workflowName, taskType, taskStatus, workflowStatus event_queue_messages_processed Counter for number of messages fetched from an event queue queueType, queueName observable_queue_error Counter for number of errors encountered when fetching messages from an event queue queueType event_queue_messages_handled Counter for number of messages executed from an event queue queueType, queueName external_payload_storage_usage Counter for number of times external payload storage was used name, operation, payloadType","title":"Publishing metrics"},{"location":"metrics/server/#collecting-metrics","text":"One way of collecting metrics is to push them into the logging framework (log4j). Log4j supports various appenders that can print metrics into a console/file or even send them to remote metrics collectors over e.g. syslog channel. Conductor provides optional modules that connect metrics registry with the logging framework. To enable these modules, configure following additional modules property in config.properties: conductor.additional.modules=com.netflix.conductor.contribs.metrics.MetricsRegistryModule,com.netflix.conductor.contribs.metrics.LoggingMetricsModule com.netflix.conductor.contribs.metrics.LoggingMetricsModule.reportPeriodSeconds=15 This will push all available metrics into log4j every 15 seconds. By default, the metrics will be handled as a regular log message (just printed to console with default log4j.properties). In order to change that, you can use following log4j configuration that prints metrics into a dedicated file: log4j.rootLogger=INFO,console,file log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.File=/app/logs/conductor.log log4j.appender.file.MaxFileSize=10MB log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n # Dedicated file appender for metrics log4j.appender.fileMetrics=org.apache.log4j.RollingFileAppender log4j.appender.fileMetrics.File=/app/logs/metrics.log log4j.appender.fileMetrics.MaxFileSize=10MB log4j.appender.fileMetrics.MaxBackupIndex=10 log4j.appender.fileMetrics.layout=org.apache.log4j.PatternLayout log4j.appender.fileMetrics.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.logger.ConductorMetrics=INFO,console,fileMetrics log4j.additivity.ConductorMetrics=false This configuration is bundled with conductor-server in file: log4j-file-appender.properties and can be utilized by setting env var: LOG4J_PROP=log4j-file-appender.properties This variable is used by startup.sh script.","title":"Collecting metrics"},{"location":"metrics/server/#integration-with-logstash-using-a-log-file","text":"The metrics collected by log4j can be further processed and pushed into a central collector such as ElasticSearch. One way of achieving this is to use: log4j file appender - logstash - ElasticSearch. Considering the above setup, you can deploy logstash to consume the contents of /app/logs/metrics.log file, process it and send further to elasticsearch. Following configuration needs to be used in logstash to achieve it: pipeline.yml: - pipeline.id: conductor_metrics path.config: /usr/share/logstash/pipeline/logstash_metrics.conf pipeline.workers: 2 logstash_metrics.conf input { file { path = [ /conductor-server-logs/metrics.log ] codec = multiline { pattern = ^%{TIMESTAMP_ISO8601} negate = true what = previous } } } filter { kv { field_split = , include_keys = [ name , type , count , value ] } mutate { convert = { count = integer value = float } } } output { elasticsearch { hosts = [ elasticsearch:9200 ] } } Note: In addition to forwarding the metrics into ElasticSearch, logstash will extract following fields from each metric: name, type, count, value and set proper types","title":"Integration with logstash using a log file"},{"location":"metrics/server/#integration-with-fluentd-using-a-syslog-channel","text":"Another example of metrics collection uses: log4j syslog appender - fluentd - prometheus. In this case, a specific log4j properties file needs to be used so that metrics are pushed into a syslog channel: log4j.rootLogger=INFO,console,file log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.File=/app/logs/conductor.log log4j.appender.file.MaxFileSize=10MB log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n # Syslog based appender streaming metrics into fluentd log4j.appender.server=org.apache.log4j.net.SyslogAppender log4j.appender.server.syslogHost=fluentd:5170 log4j.appender.server.facility=LOCAL1 log4j.appender.server.layout=org.apache.log4j.PatternLayout log4j.appender.server.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.logger.ConductorMetrics=INFO,console,server log4j.additivity.ConductorMetrics=false And on the fluentd side you need following configuration: source @type prometheus /source source @type syslog port 5170 bind 0.0.0.0 tag conductor parse ; only allow TIMER metrics of workflow execution and extract tenant ID @type regexp expression /^.*type=TIMER, name=workflow_execution.class-WorkflowMonitor.+workflowName-(? tenant .*)_(? workflow .+), count=(? count \\d+), min=(? min [\\d.]+), max=(? max [\\d.]+), mean=(? mean [\\d.]+).*$/ types count:integer,min:float,max:float,mean:float /parse /source filter conductor.local1.info @type prometheus metric name conductor_workflow_count type gauge desc The total number of executed workflows key count labels workflow ${workflow} tenant ${tenant} user ${email} /labels /metric metric name conductor_workflow_max_duration type gauge desc Max duration in millis for a workflow key max labels workflow ${workflow} tenant ${tenant} user ${email} /labels /metric metric name conductor_workflow_mean_duration type gauge desc Mean duration in millis for a workflow key mean labels workflow ${workflow} tenant ${tenant} user ${email} /labels /metric /filter match ** @type stdout /match With above configuration, fluentd will: - Listen to raw metrics on 0.0.0.0:5170 - Collect only workflow_execution TIMER metrics - Process the raw metrics and expose 3 prometheus specific metrics - Expose prometheus metrics on http://fluentd:24231/metrics","title":"Integration with fluentd using a syslog channel"}]}